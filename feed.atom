<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>http://blog.peterritchie.com/</id>
	<title>Peter Ritchie's Blog</title>
	<link rel="self" href="http://blog.peterritchie.com/" />
	<rights>2024</rights>
	<updated>2024-08-13T13:29:43Z</updated>
	<subtitle>Peter Ritchie</subtitle>
	<entry>
		<id>http://blog.peterritchie.com/posts/what-are-the-proposed-csharp-type-unions-and-how-do-they-relate-to-discriminated-unions</id>
		<title>What Are the Proposed C# Type Unions and How Do They Relate to Discriminated Unions?</title>
		<link href="http://blog.peterritchie.com/posts/what-are-the-proposed-csharp-type-unions-and-how-do-they-relate-to-discriminated-unions" />
		<updated>2024-08-13T00:00:00Z</updated>
		<content>&lt;!--what-are-the-proposed-csharp-type-unions-and-how-do-they-relate-to-discriminated-unions--&gt;
&lt;p&gt;&lt;img src="../assets/views-through-different-lenses.jpg" class="img-fluid" alt="views through different lenses" /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/dotnet/csharplang/blob/18a527bcc1f0bdaf542d8b9a189c50068615b439/proposals/TypeUnions.md"&gt;rumor&lt;/a&gt; is that a &lt;em&gt;type union&lt;/em&gt; feature may make its way into the next version of C#. The next version of C# will be 13, and C# has been around since 2000. What do type unions mean to C#?&lt;/p&gt;
&lt;p&gt;The concepts behind type unions have probably been around as long as programming languages. Some languages refer to them with different terms. Ada, Pascal, and Modula-2 refer to them as &lt;em&gt;variant records&lt;/em&gt; (like &lt;em&gt;variants&lt;/em&gt; in COM), Algo refers to them as &lt;em&gt;united modes&lt;/em&gt;, F#, and CORBA refer to them as discriminated unions and other languages refer to them as tagged unions (like Python and TypeScript). Some languages have a discriminator (tag) support in their enumerations feature (like Swift, Rust, and Haxe). You might also see discriminated union-&lt;em&gt;like&lt;/em&gt; support in other technologies like Swagger and OpenAPI.&lt;/p&gt;
&lt;p&gt;If you're familiar with C/C++, you may be familiar with unions; they're similar--their data types that define the type (or a block of memory) can be one of multiple types. C/C++ unions don't have a tag or discriminator or track what the union's storage &lt;em&gt;means&lt;/em&gt;). So, the code that uses the union has to figure out how to access data in the union correctly. I.e., the code that uses a union C/C++ must have its own discriminator/tag.&lt;/p&gt;
&lt;p&gt;C# never truly had the concept of a C/C++ union because of increased memory safety—accessing memory in many different ways wasn't considered &lt;em&gt;safe&lt;/em&gt;. You can fake C/C++ style unions in C# with &lt;code&gt;StructLayoutAttribute&lt;/code&gt; and &lt;code&gt;LayoutKind.Explicit&lt;/code&gt;. But I, for one, wouldn't recommend implementing discriminated unions that way.&lt;/p&gt;
&lt;p&gt;There have been some quasi-discriminated union implementations out there like &lt;code&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt; and &lt;code&gt;Optional&amp;lt;T&amp;gt;&lt;/code&gt;. I call these &lt;em&gt;quasi&lt;/em&gt; because they're very special-case. Most &lt;code&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt; implementations are discriminated unions because of the &lt;code&gt;Status&lt;/code&gt; or &lt;code&gt;IsSuccess&lt;/code&gt; properties that allow a consumer of an instance to tell whether to access the success or the error information. &lt;code&gt;Optional&amp;lt;T&amp;gt;&lt;/code&gt; is like &lt;code&gt;Nullable&amp;lt;T&amp;gt;&lt;/code&gt; with a discriminator &lt;code&gt;HasValue&lt;/code&gt;, but more semantically aligned with an &lt;em&gt;optional value&lt;/em&gt; (that could also be nullable).&lt;/p&gt;
&lt;h2 id="will-c-get-discriminated-unions-or-type-unions"&gt;Will C# Get &lt;em&gt;Discriminated Unions&lt;/em&gt; or &lt;em&gt;Type Unions&lt;/em&gt;?&lt;/h2&gt;
&lt;p&gt;The proposal for C# uses &amp;quot;Type Union&amp;quot; because the discriminator isn't explicit. The union object (or, more accurately, the C# compiler) knows the instance type and allows the use of existing C# type-matching features (like &lt;code&gt;is&lt;/code&gt;). The proposed syntax for declaring a Type Union is similar to declaring a single-level inheritance hierarchy that cannot be inherited from and behaves as if it is &lt;em&gt;closed&lt;/em&gt;. In fact, BCL support for the concept will use a &lt;code&gt;ClosedAttribute&lt;/code&gt; to declare Type Unions in a Common Type System way.&lt;/p&gt;
&lt;p&gt;Without a Type Union feature, something with a discriminator (specifically something like &lt;a href="https://blog.nimblepros.com/blogs/getting-started-with-ardalis-result/"&gt;&lt;code&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt;, is used like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;// makes use of predefined union &amp;quot;Result&amp;lt;T&amp;gt;&amp;quot;
public Result&amp;lt;Resource&amp;gt; GetResource(Guid id)
{
   var resource = _context.Resources.SingleOrDefault(e =&amp;gt; e.Id == id);
   if(resource == null) return new Result&amp;lt;Resource&amp;gt;.NotFound;
   return new Result&amp;lt;Resource&amp;gt;(resource);
}

//... 

var result = GetResource(id);
if(result.Status == ResultStatus.Ok)
{
  result.Value.Description = newDescription;
  UpdateResource(result.Value);
}
else
{
  CreateResource(new Resource(id){Description = newDescription};
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the proposed C# syntax, you'd declare a type union something like this, and for the point of comparison, I'll do one possible, simplified Type Union implementation of &lt;code&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public union struct Result&amp;lt;TValue&amp;gt;
{
  Success(TValue value);
  Failure(ErrorCode errorCode);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a &lt;em&gt;Type Union&lt;/em&gt;, this declares a union &lt;code&gt;Result&lt;/code&gt; and member types &lt;code&gt;Success&lt;/code&gt; and &lt;code&gt;Failure&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Creating an instance is similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;Result&amp;lt;Resource&amp;gt; result = Succes(resource);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;Result&amp;lt;Resource&amp;gt; result = Failure(ErrorCode.NotFound);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instantiating one of the types and assigning it to an instance of the union—much like assigning a subclass to an instance of the base class.&lt;/p&gt;
&lt;p&gt;And use the instance like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;if(result is Failure f) { /*...*/ }
&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NOTE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;These examples were written without access to the feature in the language. Based solely on documentation, expect syntax errors when the feature is released.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This example is covered in some common patterns in the proposal and only covers a part of what's possible with Type Unions (that part is &lt;em&gt;struct unions&lt;/em&gt;.) What's nice about this proposed feature is that it also considers union classes, ad hoc unions, and a limited ability to create custom unions (e.g., &lt;code&gt;ClosedAttribute&lt;/code&gt; and &lt;code&gt;UnionAttribute&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;What's also nice about this proposal is that it considers records, refs, boxing, nullability, co-/contra-variance (with ad hoc unions), exhaustiveness, nullability, equivalence, and assignability. The proposal might only do &lt;em&gt;most&lt;/em&gt; of what you'd expect, but it does what is correct and useful.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/views-through-different-lenses.jpg" class="img-fluid" alt="views through different lenses"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/aspiring-links-2024-07-14</id>
		<title>Aspiring Links for Period Ending July 14, 2024</title>
		<link href="http://blog.peterritchie.com/posts/aspiring-links-2024-07-14" />
		<updated>2024-07-14T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="../assets/aspiring-links.png" class="img-fluid" alt="A made from chain links" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://visualstudiomagazine.com/Articles/2024/06/28/net-aspire.aspx"&gt;Visual Studio Magazine - Microsoft Making Big .NET Aspire Push, So What Is It?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/live/YXCqkJLL3XE?si=MmW-KmBtkQnXvmXi"&gt;YouTube - Learn C# with CSharpFritz: Basics of .NET Aspire - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/hTtlBlsy1WA?si=nQYJggHvNg4gNkXx"&gt;YouTube - What is Dev Time Orchestration in .NET Aspire?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/fYqnWmhR-HU?si=vQogEpoNDNbmawhc"&gt;YouTube - Meet .NET Aspire&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/qtUgntF21Uw?si=WsVnZ2UPBWPw3dN-"&gt;YouTube - What are .NET Aspire components?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/lCzQ2yQr6JE?si=b3WEiW1ZX6lYjVY0"&gt;YouTube - How to deploy .NET Aspire applications?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/_DzGe2DXrKk?si=hdIRJaoUKiJrSS6h"&gt;YouTube - What is .NET Aspire telemetry?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/DzUhBG7uKUk?si=Sto7FpG7-dG51-8n"&gt;YouTube - What are .NET Aspire service defaults?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/qT6AXCLrZtw?si=ADHbcJLQ9HoR3XM1"&gt;YouTube - Technology and Friends - Scott Hunter on .NET Aspire&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/live/PUCU9ZOOgQ8?si=OgYaSvjqYPDpAJMr"&gt;YouTube - Let's Learn .NET Aspire (Português)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/live/48CWnYfTZhk?si=cgzeQ7kiEBmvS-s_"&gt;YouTube - Let's Learn .NET: Aspire (Tiếng Việt)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.postsharp.net/aspire-caching-metalama"&gt;Metalama - Simplify Your .NET Aspire Caching With Metalama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jeffreyfritz.com/2024/07/adding-antivirus-to-net-aspire-systems/"&gt;Fritz on the Web - Adding Antivirus to .NET Aspire Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/aspiring-links.png" class="img-fluid" alt="A made from chain links"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/aspiring-links-2024-06-14</id>
		<title>Aspiring Links for Week Ending June 14, 2024</title>
		<link href="http://blog.peterritchie.com/posts/aspiring-links-2024-06-14" />
		<updated>2024-06-14T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="../assets/aspiring-links.png" class="img-fluid" alt="A made from chain links" /&gt;&lt;/p&gt;
&lt;p&gt;Using PostgreSQL with .NET and Entra ID (Blog)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://devblogs.microsoft.com/dotnet/using-postgre-sql-with-dotnet-and-entra-id/"&gt;https://devblogs.microsoft.com/dotnet/using-postgre-sql-with-dotnet-and-entra-id/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Official .NET Aspire Learning Path at Microsoft Learn (Site)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/training/paths/dotnet-aspire/"&gt;https://learn.microsoft.com/en-us/training/paths/dotnet-aspire/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My First look at .NET Aspire. What's with the Hype? by CodeOpinion (Video)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://youtu.be/maVXnkYEDIE?si=7AkKulcvVOgnxmVz"&gt;https://youtu.be/maVXnkYEDIE?si=7AkKulcvVOgnxmVz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Instant OpenTelemetry Dashboard for JavaScript Developers with Aspire (Video)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://youtu.be/YKraN1ZETpw?si=XZUWcnZMtpzl7ztT"&gt;https://youtu.be/YKraN1ZETpw?si=XZUWcnZMtpzl7ztT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;.NET Aspire: Your Gateway to Cloud-Native Success (Blog)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://riasolutionsgroup.com/2024/06/11/using-dynatrace-to-navigate-the-microservices-maze/"&gt;https://riasolutionsgroup.com/2024/06/11/using-dynatrace-to-navigate-the-microservices-maze/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Aspirant - Extensions for .NET Aspire (GitHub Repo)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/aspirant-project/aspirant"&gt;https://github.com/aspirant-project/aspirant&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/aspiring-links.png" class="img-fluid" alt="A made from chain links"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/new-and-proposed-changes-in-csharp-13-2024-jun-5</id>
		<title>New And Proposed Changes For C# 13 - June 5, 2024</title>
		<link href="http://blog.peterritchie.com/posts/new-and-proposed-changes-in-csharp-13-2024-jun-5" />
		<updated>2024-06-05T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="../assets/types-of-extensions.png" class="img-fluid" alt="Extensions, extensions, extensions" /&gt;&lt;/p&gt;
&lt;p&gt;C# 13 is shaping up to introduce some interesting and useful additions to the C# language. Let's look at what we know as of June 5, 2024.&lt;/p&gt;
&lt;h2 id="extension-types"&gt;Extension Types&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Extension Types&lt;/em&gt; (announced at Build 2024) are the latest proposed addition to C# 13. Microsoft introduced &lt;em&gt;Extension Methods&lt;/em&gt; in C# 3, and since C# 4, the concept of &amp;quot;Extension Everything&amp;quot; has been an anticipated addition. &lt;em&gt;Extension Methods&lt;/em&gt; added the ability to extend non-static classes with statically-defined methods in unrelated static classes. &lt;em&gt;Extension Types&lt;/em&gt; extend the extensibility that Extension Methods offer, a huge leap towards Extension Everything.&lt;/p&gt;
&lt;p&gt;Extension Methods syntax stays the same, but you can be clearer about the intent of your extension methods by declaring implicit extension classes.&lt;/p&gt;
&lt;p&gt;An extension method in C#3-C#12 may look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public static class StringExtensions
{
    public static bool IsNotNullOrEmpty(this string text)
    {
        return !string.IsNullOrEmpty(text);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code introduces the ability to do something like &lt;code&gt;myText.IsNotNullOrEmpty()&lt;/code&gt;. In C# 13, you can be more direct (I'm trying to avoid using the word &amp;quot;explicit&amp;quot;; you'll see why in a sec.) by declaring an &lt;em&gt;implicit extension&lt;/em&gt; class with what looks like an instance method &lt;code&gt;IsNotNullOrEmpty&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;implicit extension StringExtensions for string
{
    public bool IsNotNullOrEmpty()
    {
        return !string.IsNullOrEmpty(this);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This alternative way to re-implement an extension method implements an instance method on the extension class—which is how the extension method appears when used: a call to a method on an instance of a type (&lt;code&gt;string&lt;/code&gt; in our example.) We're not limited to extending the &lt;em&gt;instance&lt;/em&gt; of a type; we can also extend the type with static methods. &lt;code&gt;IsNotNullOrEmpty&lt;/code&gt; is looking to be the opposite of the static &lt;code&gt;string&lt;/code&gt; method &lt;code&gt;IsNullOrEmpty&lt;/code&gt;. With Extension Types we will be able to implement &lt;code&gt;IsNotNullOrEmpty&lt;/code&gt; as a static method on &lt;code&gt;string&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;implicit extension StringExtensions for string
{
    public static bool IsNotNullOrEmpty(string text)
    {
        return !string.IsNullOrEmpty(text);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code adds a &lt;strong&gt;static extension method&lt;/strong&gt; that allows for &lt;code&gt;string.IsNotNullOrEmpty(myText)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But, since the code will truly be in the context—or scope of—&lt;code&gt;string&lt;/code&gt;, I don't need to qualify the call to &lt;code&gt;IsNullOrEmpty&lt;/code&gt; with the type &lt;code&gt;string&lt;/code&gt;! So, we could do this instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;implicit extension StringExtensions for string
{
    public static bool IsNotNullOrEmpty(string text)
    {
        return !IsNullOrEmpty(text);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this is &amp;quot;Extensions Types,&amp;quot; we will also have &lt;em&gt;extension properties&lt;/em&gt;. Continuing with my theme of IsNotNullOrEmpty (potentially in a controversial way), we could implement my original extension method as an extension property:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;implicit extension StringExtensions for string
{
    public bool IsNotNullOrEmpty =&amp;gt; !IsNullOrEmpty(this);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code allows us to write &lt;code&gt;myText.IsNotNullOrEmpty&lt;/code&gt;. (Feel free to comment that I'm mad for making a property on a potentially &lt;code&gt;null&lt;/code&gt; instance of a &lt;code&gt;string&lt;/code&gt;, this is just an example.)&lt;/p&gt;
&lt;p&gt;Extension Types isn't limited to extension methods and properties but also includes extension indexers. Stealing from Dustin and Mads' demo, let's say we want to provide access to individual bits with a &lt;code&gt;UInt64&lt;/code&gt;; I could write an implicit extension type like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public implicit extension Bits for Uint64)
{
    public bool this[int index]
    {
        get =&amp;gt; (this &amp;amp; Mask(index)) != 0;
        set =&amp;gt; this = value ? this | Mask(value) : this &amp;amp; !Mash(index);
    }
    
    static ulong Mask(int index) =&amp;gt; `ul &amp;lt;&amp;lt; index;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;... so I could obtain the first bit of an unsigned 64-bit number with &lt;code&gt;unsigned64BitNumber[0]&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;What's with &lt;code&gt;implicit&lt;/code&gt;? Like implicit operators that will be resolved to based on inferred scope or that it &lt;em&gt;implicitly&lt;/em&gt; applies to all cases of the underlying type (&lt;code&gt;... for &amp;lt;type&amp;gt;&lt;/code&gt;.) The keyword &lt;code&gt;explicit&lt;/code&gt; means a caller can only use the member in an explicitly-qualified scope. So, if I made my &lt;code&gt;StringExtensions&lt;/code&gt; class &lt;code&gt;explicit&lt;/code&gt; instead, the use of &lt;code&gt;IsNullOrEmpty&lt;/code&gt; would only be callable by qualifying with &lt;code&gt;StringExtensions&lt;/code&gt;: &lt;code&gt;(StringExtensions)myText.IsNotNullOrEmpty&lt;/code&gt;. At least, that's what's currently proposed.&lt;/p&gt;
&lt;p&gt;Extension Types is a work in progress, so some of these things may change—I expect details around &lt;code&gt;explicit&lt;/code&gt; usage to change.&lt;/p&gt;
&lt;h2 id="new-field-contextual-keyword"&gt;New &lt;code&gt;field&lt;/code&gt; Contextual Keyword&lt;/h2&gt;
&lt;p&gt;A feature that almost made it into C# 12 is the &lt;code&gt;field&lt;/code&gt; contextual keyword--contextual in autoproperties. &lt;code&gt;field&lt;/code&gt; is an &amp;quot;alias&amp;quot; got access the auto-generated backing field.&lt;/p&gt;
&lt;p&gt;Since you could declare a variable within the context of an autoproperty, &lt;code&gt;field&lt;/code&gt; is technically a breaking change.&lt;/p&gt;
&lt;h2 id="params-collections"&gt;&lt;code&gt;params&lt;/code&gt; Collections&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;params&lt;/code&gt; has existed in C# since the start as a means of declaring a method that takes a variable number of arguments. Generic types were not available at the time, so &lt;code&gt;params&lt;/code&gt; only works with &lt;em&gt;arrays&lt;/em&gt;, for example, &lt;code&gt;void Method(params string[] args)&lt;/code&gt; or &lt;code&gt;void Method(params int[] numbers)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This means that &lt;code&gt;ReadOnlySpan&amp;lt;T&amp;gt;&lt;/code&gt; may be used with &lt;code&gt;params&lt;/code&gt; to realize the performance improvements of spans.&lt;/p&gt;
&lt;h2 id="system.threading.lock-type-and-lock"&gt;&lt;code&gt;System.Threading.Lock&lt;/code&gt; Type and &lt;code&gt;lock&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;.NET 9 introduces an improved lock type &lt;code&gt;System.Threading.Lock&lt;/code&gt;. &lt;code&gt;System.Threading.Lock&lt;/code&gt; offers improvements over &lt;code&gt;Monitor&lt;/code&gt; in that &lt;code&gt;Lock&lt;/code&gt; supports a narrower locking scope and avoids the overhead of SyncBlock.&lt;/p&gt;
&lt;p&gt;C# 13 &lt;code&gt;lock&lt;/code&gt; recognizes if the target is &lt;code&gt;System.Threading.Lock&lt;/code&gt; and uses its updated API.&lt;/p&gt;
&lt;h2 id="new-escape-sequence-for-esc-character"&gt;New Escape Sequence For &lt;kbd&gt;Esc&lt;/kbd&gt; Character&lt;/h2&gt;
&lt;p&gt;You can now enter &lt;kbd&gt;Esc&lt;/kbd&gt; as a string/character literal with &lt;code&gt;'\e'&lt;/code&gt;. Prior to C# 13, having the &lt;kbd&gt;Esc&lt;/kbd&gt; character in a string would require typing &lt;code&gt;\u001b&lt;/code&gt; (or &lt;code&gt;\x1b&lt;/code&gt;, but that's not recommended.) I believe this accepts the existing Regular Expression character sequence of &lt;code&gt;\e&lt;/code&gt; as an escape for &lt;kbd&gt;Esc&lt;/kbd&gt;.&lt;/p&gt;
&lt;h2 id="improved-overload-resolution-for-method-groups"&gt;Improved Overload Resolution For Method Groups&lt;/h2&gt;
&lt;p&gt;C# 13 will improve overload resolution of method groups when determining the method group's natural type. Oversimplication: instead of generating all possible candidate methods and selecting the best candidate, candidates will be constructed scope by scope. If, after pruning all the candidates at one scope, candidates at the next scope are constructed and considered. If you're interested at that level, see &lt;a href="https://github.com/dotnet/roslyn/issues/69222"&gt;Generic constraints should prune out candidates for method group natural type&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="index-operator-now-available-in-object-initializer-expressions"&gt;Index operator &lt;code&gt;^&lt;/code&gt; Now Available In Object Initializer Expressions&lt;/h2&gt;
&lt;p&gt;You can now use the &amp;quot;From the end&amp;quot; operator in object initializer expressions. e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var countdown = new TimerRemaining()
{
    buffer =
    {
        [^1] = 0,
        [^2] = 1,
        [^3] = 2,
        [^4] = 3,
        [^5] = 4,
        [^6] = 5,
        [^7] = 6,
        [^8] = 7,
        [^9] = 8,
        [^10] = 9
    }
};
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="caveat"&gt;Caveat&lt;/h2&gt;
&lt;p&gt;&amp;quot;C# 13&amp;quot; is a work in progress, and the above was accurate when it was published. Hopefully we'll get everything detailed above but we may not. We may not get some of these features or we may get more!&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dotnet/roslyn/issues/11159"&gt;Language Feature: Extension Everything&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dotnet/csharplang/issues/5497"&gt;[Proposal]: Extensions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dotnet/csharplang/issues/140"&gt;Proposal: Semi-Auto-Properties; field keyword&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/proposals/params-collections"&gt;&lt;code&gt;params&lt;/code&gt; Collections Feature Proposal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/proposals/lock-object"&gt;Lock Object Feature Specification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dotnet/csharplang/issues/7400"&gt;[Proposal]: String/Character escape sequence \e as a short-hand for \u001b (&amp;lt;ESCAPE&amp;gt;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/proposals/method-group-natural-type-improvements"&gt;Method group natural type improvements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dotnet/csharplang/issues/7429"&gt;[Proposal]: Method group natural type improvements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dotnet/csharplang/issues/7684"&gt;Open issue: evaluation of implicit indexers in object initializers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/types-of-extensions.png" class="img-fluid" alt="Extensions, extensions, extensions"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/dealing-with-primitive-obsession-with-entity-framework-core</id>
		<title>Dealing with Primitive Obsession with Entity Framework Core</title>
		<link href="http://blog.peterritchie.com/posts/dealing-with-primitive-obsession-with-entity-framework-core" />
		<updated>2024-05-22T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="../assets/obsessed-with-primitive-shapes.png" class="img-fluid" alt="obsessed only with creating with primitive shapes" /&gt;&lt;/p&gt;
&lt;p&gt;Unlike object-oriented languages, serialization and data transfer are limited to a small set of primitive types. These primitive types are often text, numbers (integers, floating point), booleans, arrays, etc. Sometimes, you're fortunate enough to have a more modern framework and have fancy types like date, time, and date/time, UUID/GUID, monetary, or binary data.&lt;/p&gt;
&lt;p&gt;Primitive types are &lt;em&gt;value types&lt;/em&gt;—they don't have identity semantics, and their equality/equivalence is based on their &lt;em&gt;value&lt;/em&gt;. Entity Framework Core supports &lt;em&gt;value types&lt;/em&gt; as attributes but only insomuch as what the underlying provider supports. Any complex types are &amp;quot;entities&amp;quot; in Entity Framework, despite not having an &amp;quot;identity&amp;quot;.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Note&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Every type that Entity Framework is configured to recognize is an &lt;em&gt;Entity&lt;/em&gt; (e.g., via &lt;code&gt;ModelBuilder.Entity&amp;lt;T&amp;gt;()&lt;/code&gt; or &lt;code&gt;IEntityTypeConfiguration&amp;lt;T&amp;gt;&lt;/code&gt;), despite an entity traditionally (English definition, philosophically, ontologically, or even entity-relationship diagrams) being an individual/instance with an independent (of its attributes) existence.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Domains invariably have many value types. Some may be complex (multiple attributes), and some may be simple (subsets of primitive types.) Even &lt;em&gt;simple&lt;/em&gt; domain value types have unique semantics—that constrain value validity. A common value type &lt;em&gt;Name&lt;/em&gt;, for example, is textual and thus can be implemented with a textual primitive type (like &lt;code&gt;string&lt;/code&gt; or &lt;code&gt;VARCHAR&lt;/code&gt;.) A textual primitive type like &lt;code&gt;string&lt;/code&gt; is a sequence of characters up to 2,146,483,647 characters of any value. This is unrealistic for a &amp;quot;name&amp;quot; in most domains: e.g., a name over 2 billion characters in length or containing characters like &lt;code&gt;%'&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, or &lt;code&gt;&amp;gt;&lt;/code&gt; is problematic. Unlike how an integer is not a valid text value, and there exists no way to assign an integer to a string (e.g., &lt;code&gt;string text = 1;&lt;/code&gt; is not valid and cannot be compiled and thus never be executed), re-using a primitive type for &lt;em&gt;domain types&lt;/em&gt; with validity (consistency) constraints means instances of these domain values may not be valid (e.g., &lt;code&gt;name = &amp;quot;%&amp;quot;&lt;/code&gt; or &lt;code&gt;name = new string('&amp;lt;', int.MaxValue);&lt;/code&gt;) until being checked. Enter &lt;strong&gt;Primitive Obsession&lt;/strong&gt;.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Hint&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Entity Framework &amp;quot;entities&amp;quot; are not the same as Domain-Driven Design &amp;quot;entities.&amp;quot; Despite being based on relational entities, Entity Framework considers groups of nested (owned) attributes to be entities even if they don't map to their own table (with key.)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="primitive-obsession"&gt;Primitive Obsession&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Primitive Obsession&lt;/em&gt; is a type of code smell&lt;span title="Fowler, M. (2018). &amp;quot;Refactoring: Improving the Design of Existing Code&amp;quot; "&gt;&lt;sup&gt;&lt;a href="https://amzn.to/3KkcnDT"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; where—in an object-oriented context—something that realistically  has unique behavior and semantics has been implemented with a primitive type that doesn't guard that concept's invariants. Primitive Obsession recommends that invariants be abstracted within a unique type so that invalid or inconsistent instances of that type cannot exist.&lt;/p&gt;
&lt;h3 id="value-object-mapping-in-entity-framework"&gt;Value Object Mapping in Entity Framework&lt;/h3&gt;
&lt;p&gt;Fortunately, Entity Framework Core supports mapping from domain-specific value types to database primitive types. The domain-specific types then maintain their consistency, and correct usage is enforced.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Note&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;The converse is also important; not all uses of primitive types need to be abstracted away within a custom domain type. For example, a domain type may have attributes that realistically won't have values outside a specific range, but a type does not need to be created to abstract a primitive type. For example, making a &lt;code&gt;Length&lt;/code&gt; value type that wraps an integer value that provides no added value increases complexity rather than reduces it.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I've encountered some complex domains that treat identity very specifically. In cases like this, custom value types like &lt;em&gt;Identifier&lt;/em&gt; that may use one of many primitive types are created to abstract the underlying primitive type. For example, a high-level view of an Identifier class (that doesn't include details like validation, parsing, etc.):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public record Identifier&amp;lt;T&amp;gt; where T : struct, IEquatable&amp;lt;T&amp;gt;
{
    private readonly T _value;
    public Identifier(T value) =&amp;gt; _value = value;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Implemented as a &lt;code&gt;record&lt;/code&gt;, &lt;code&gt;Identifer&amp;lt;T&amp;gt;&lt;/code&gt; operates as a value type (&lt;code&gt;struct&lt;/code&gt;, at the runtime level to implement a domain-level &lt;em&gt;value type&lt;/em&gt;) and provides equality and isolation (&lt;code&gt;Identifier&amp;lt;int&amp;gt;&lt;/code&gt; is not the same as &lt;code&gt;Identifier&amp;lt;Guid&amp;gt;&lt;/code&gt;. They cannot be equal, or compared.)&lt;/p&gt;
&lt;p&gt;Inferring the mapping of instances of this type to primitive database column types isn't possible (and EF will let you know with an &lt;code&gt;InvalidOperationException&lt;/code&gt; detailing &lt;em&gt;The 'Identifier' property 'ShippingCompany.Identifier' could not be mapped because the database provider does not support this type.&lt;/em&gt; We need to tell Entity Framework Core how to map to a type the database provider supports. This is done with the &lt;code&gt;PropertyBuilder&amp;lt;TProperty&amp;gt;.HasConversion&lt;/code&gt; method when configuring the containing entity type.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;protected override void OnModelCreating(ModelBuilder modelBuilder)
{
    modelBuilder
        .Entity&amp;lt;ShippingCompany&amp;gt;()
        .Property(e =&amp;gt; e.Identifer)
        .HasConversion(identifier =&amp;gt; identifer.Value, value =&amp;gt; new Identifier&amp;lt;Guid&amp;gt;(value));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above shows how you can address Primitive Obsession when using Entity Framework but, it assumes that you're directly persisting domain objects through Entity Framework. Focusing on behavior over attributes in a domain may not make all domain objects persistable by Entity Framework. Still, you can at least push off the mapping of non-primitive value types to Entity Framework if you don't want to introduce another layer of mapping (treating the types persisted via Entity Framework as persistence DTOs).  You could still have a non-primitive-DTO (e.g., &lt;code&gt;IdentifierDto&lt;/code&gt;) persisted through Entity Framework.  I'll leave it up to you to decide if that's &amp;quot;proper&amp;quot; for your domain.&lt;/p&gt;
&lt;h3 id="references"&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[1]: &lt;a href="https://amzn.to/3KkcnDT"&gt;Refactoring: Improving the Design of Existing Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiki.c2.com/?ReplaceDataValueWithObject"&gt;Replace Data Value With Object Refactoring&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/obsessed-with-primitive-shapes.png" class="img-fluid" alt="obsessed only with creating with primitive shapes"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/contributing-to-many-git-repos</id>
		<title>A tool to help contributing to many Git repos</title>
		<link href="http://blog.peterritchie.com/posts/contributing-to-many-git-repos" />
		<updated>2023-11-30T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="../assets/contributing-to-many-git-repos.jpg" class="img-fluid" alt="Source code from many sources" /&gt;&lt;/p&gt;
&lt;p&gt;I've contributed to many Git repos over the years. Doing this means I work in a code base for a little while, switch to another, and often eventually switch back.&lt;/p&gt;
&lt;h2 id="collaborating-with-others"&gt;Collaborating with Others&lt;/h2&gt;
&lt;p&gt;In the repos that I work in, many have multiple contributors. The contributions to those repos can be prolific, and if the repo is using a workflow that uses feature or topic branches, branches come and go quite often. &lt;code&gt;git fetch&lt;/code&gt; by default (or with no other options) gets all branches so you'll have other team members' branches after a fetch--which can be used to do a deep dive on a PR.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;You could choose not to use the &lt;code&gt;git fetch&lt;/code&gt; defaults and have it only get a particular branch. This can typically be done with &lt;code&gt;git fetch origin main&lt;/code&gt; (depending on how you've named your remotes and your branches.)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;p&gt;I work with many organizations and rarely is there one repo (yes, I know, there's this thing called a &amp;quot;monorepo&amp;quot;; but I find that organizations that can make this work need to be very technically savy, with products/technologies geared towards developers, and only a few of the organizations I work with are at that level.) With remote work being what it is (I'm often working at a different time than other contributors), when I return to work with an organization's code, I usually need to update several repos.&lt;/p&gt;
&lt;p&gt;|Why not do a &lt;code&gt;git pull&lt;/code&gt; instead of &lt;code&gt;git fetch&lt;/code&gt;?|
|:-:|
What I'm contributing to, what I may be reviewing, and whether I'm connected, are variable enough that I've built a habit only to pull when I'm ready to merge and deal with potential conflicts. If I have conflicts, I must resolve them (or abort: &lt;code&gt;git merge --abort&lt;/code&gt; or &lt;code&gt;git fetch origin&lt;/code&gt; and &lt;code&gt;git reset --hard origin&lt;/code&gt;) before doing anything else. This means I must commit to resolving those conflicts before switching to another branch to review or work with it. (Yes, I could re-clone in a different place, but frequent-fetch&amp;gt;abort&amp;gt;clone in terms of effort and risk.)}&lt;/p&gt;
&lt;h1 id="a-tool-to-help"&gt;A tool to help&lt;/h1&gt;
&lt;p&gt;When I re-start work (or maybe I'm coming off a vacation), going to each repo dir to perform &lt;code&gt;git fetch&lt;/code&gt; is tedious. I've developed a Powershell script to do that. I'll walk through the script after the code (commented code available &lt;a href="https://github.com/peteraritchie/pri.powershell/blob/main/git/fetch-all.ps1"&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;using namespace System.IO;
param (
    [switch]$WhatIf,
    [switch]$Verbose,
    [switch]$Quiet
    )
$currentDir = (get-location).Path;

if($Verbose.IsPresent) {
    $VerbosePreference = &amp;quot;Continue&amp;quot;;
}

function Build-Command {
    $expression = 'git fetch';
    if($Quiet.IsPresent) {
        $expression += ' -q';
    }
    if($Verbose.IsPresent) {
        $expression += ' -v --progress';
    }
    if($WhatIf.IsPresent) {
        $expression += ' --dry-run';
    }
    $expression += &amp;quot; origin&amp;quot;;
    return $expression;
}

foreach($item in $([Directory]::GetDirectories($currentDir, '.git', [SearchOption]::AllDirectories);)) {
    $dir = get-item -Force $item;
    Push-Location $dir.Parent;
    try {
        Write-Verbose &amp;quot;fetching in $((Get-Location).Path)...&amp;quot;;
        $expression = Build-Command;

        Invoke-expression $expression;

    } finally {
        Pop-Location;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, I'm translating the PowerShell idioms &lt;code&gt;WhatIf&lt;/code&gt;, &lt;code&gt;Verbose&lt;/code&gt;, and &lt;code&gt;Quiet&lt;/code&gt; to common Git options &lt;code&gt;--dry-run&lt;/code&gt;, &lt;code&gt;--verbose&lt;/code&gt; (&lt;code&gt;-v&lt;/code&gt;), and &lt;code&gt;--quiet&lt;/code&gt; (&lt;code&gt;-q&lt;/code&gt;). The &lt;code&gt;Build-Command&lt;/code&gt; builds up the expression we want to use to invoke git. I've included the &lt;code&gt;--progress&lt;/code&gt; option with &lt;code&gt;git fetch&lt;/code&gt; to display progress when&lt;code&gt;-Verbose&lt;/code&gt; is specified. Next, I'm looping through all directories, looking for a &lt;code&gt;.git&lt;/code&gt; directory. I'm using &lt;code&gt;System.IO.GetDirectories&lt;/code&gt; instead of &lt;code&gt;Get-ChildItem&lt;/code&gt; because it's much faster. For each directory that contains a &lt;code&gt;.git&lt;/code&gt; subdirectory, Git &lt;code&gt;fetch&lt;/code&gt; is invoked. This allows me to fetch several Git repos within the hierarchy of the current directory.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;Organizaing Code Locally&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;I work with my code (spikes, libraries, experiments, etc.), open-source projects, and multiple clients. All these diverge from one another at one level in my directory structure. e.g. I may have a &lt;code&gt;src&lt;/code&gt; subdiretory in my home directory; and &lt;code&gt;oss&lt;/code&gt;, &lt;code&gt;experiments&lt;/code&gt;, and &lt;code&gt;client&lt;/code&gt; subdirectories within &lt;code&gt;src&lt;/code&gt;, so I can choose to fetch from all the repos recursively in each of those subdirectories--if I'm returning to work on an OSS project after being away from OSS for a while, I just &lt;code&gt;fetch-all.ps&lt;/code&gt; within the &lt;code&gt;oss&lt;/code&gt; subdirectory.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By default (or with no other options), &lt;code&gt;git fetch&lt;/code&gt; does not delete corresponding local branches that have been removed from a remote. So, new branches will be downloaded, but those that were removed will remain.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;To also remove local branches removed from the remote, you can include a purge option with &lt;code&gt;git fetch&lt;/code&gt;: &lt;code&gt;git fetch --prune&lt;/code&gt; or &lt;code&gt;git fetch -p&lt;/code&gt;.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;/table&gt;
&lt;p&gt;If I'm reviewing a PR, I don't necessarily want removed remote branches to be removed locally &lt;em&gt;all the time&lt;/em&gt;. So, I like pruning separately from fetching. The following is the script for that (other than Build-Command, it has the same structure and flow as &lt;code&gt;fetch-all.ps1&lt;/code&gt; (so I won't walk through this snippet.)&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;using namespace System.IO;
param (
    [switch]$WhatIf,
    [switch]$Verbose
    )
$currentDir = (get-location).Path;

if($Verbose.IsPresent) {
    $VerbosePreference = &amp;quot;Continue&amp;quot;;
}

function Build-Command {
    $expression = 'git remote';
    if($Verbose.IsPresent) {
        $expression += ' -v';
    }
    $expression += ' prune';
    if($WhatIf.IsPresent) {
        $expression += ' --dry-run';
    }
    $expression += ' origin';
    return $expression;
}

foreach($item in $([Directory]::GetDirectories($currentDir, '.git', [SearchOption]::AllDirectories);)) {
    $dir = get-item -Force $item;
    Push-Location $dir.Parent;
    try {
        Write-Verbose &amp;quot;pruning in $((Get-Location).Path)...&amp;quot;;
        $expression = Build-Command;

        Invoke-expression $expression;
    } finally {
        Pop-Location;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Separating pruning from fetching also allows me to prune at a wider scope than fetching. e.g. &lt;code&gt;c:\Users\peter\src\client .\fetch-all.ps1&lt;/code&gt; and &lt;code&gt;c:\Users\peter\src .prune-all.ps1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I look forward to your feedback and comments.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/contributing-to-many-git-repos.jpg" class="img-fluid" alt="Source code from many sources"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/entity-framework-in-aspire</id>
		<title>Entity Framework in .NET Aspire</title>
		<link href="http://blog.peterritchie.com/posts/entity-framework-in-aspire" />
		<updated>2023-11-29T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="../assets/entity-framework-in-aspire.jpg" class="img-fluid" alt="A path through the infrastructure" /&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;.NET Aspire is an opinionated, cloud ready stack for building observable, production ready, distributed applications.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://learn.microsoft.com/en-us/dotnet/aspire/get-started/aspire-overview"&gt;.NET Aspire&lt;/a&gt; is currently in preview and focuses on &lt;em&gt;simplifying the developer experience&lt;/em&gt; with &lt;em&gt;orchestration&lt;/em&gt; and &lt;em&gt;automatic service discovery&lt;/em&gt; features. There's a huge potential for .NET Aspire beyond this initial valuable feature set.&lt;/p&gt;
&lt;p&gt;Being in preview, .NET Aspire may not yet support all the scenarios or workloads you may be comfortable with. It's an opinionated framework, which means differences of opinion are natural and expected. Currently, one of those opinions seems to be a focus on containers. The sample solutions that the new &lt;code&gt;dotnet&lt;/code&gt; templates provide are a great example of the benefits of containerization. The .NET Aspire starter solution that &lt;code&gt;dotnet new --use-redis-cache --output AspireStarter&lt;/code&gt; generates, out of the box, is something that, when debugged, will download, run, and utilize a Docker Redis image. (I've worked with teams where getting each member productive in a development environment has ended up being days of work.) The AppHost component of a .NET Aspire solution codifies abstract aspects of the architectural decisions that automates the generation and deployment of a development environment&amp;lt;(!--and configuration provides the details from future decisions about other environments--&amp;gt;.)&lt;/p&gt;
&lt;p&gt;A container focus is empowered by .NET Aspire's &lt;em&gt;orchestration&lt;/em&gt; features. An independent orchestration responsibility enables better separation of &lt;em&gt;release and deploy&lt;/em&gt; concerns from &lt;em&gt;build and test&lt;/em&gt; concerns; &lt;em&gt;shifting right&lt;/em&gt; those decisions that release and deploy depend on. (i.e., the ability to develop, execute, and evaluate solutions are discernibly &lt;em&gt;left&lt;/em&gt; of release and operation.) Containers are an established method of componentizing a distributed system with independent servers (sometimes called &amp;quot;tiers.&amp;quot;) This provides flexibility to deploy and execute in a development environment even before architectural decisions about a production topology have been considered. For example, debugging the .NET Aspire starter app automatically spins up a Redis container in Docker, but it's extremely unlikely that's how it will be deployed in production. In production, will there be one only Redis instance? If you have many instances, what sort of gateway or reverse proxy to that pool of instances will be utilized? Will it be on-prem or cloud? Will it be Azure, AWS, or Google Cloud? The beauty of Aspire's orchestration feature is that it doesn't matter yet; you can configure orchestration to &lt;em&gt;figure it out at run-time&lt;/em&gt;, one environment at a time!&lt;/p&gt;
&lt;p&gt;But, with every decision comes compromise. Technologies that depend on the physical resources that come from those decisions (that we're now effectively deferring) introduce some challenges with some existing software development idioms. A chicken-and-egg situation: if how to connect to physical resources may only be known at run-time, what happens to design-time technologies that depend on that connection information?&lt;/p&gt;
&lt;p&gt;One popular technology in .NET, Entity Framework, suffers one of those challenges in .NET Aspire (Possibly only in &lt;em&gt;code-first&lt;/em&gt; scenarios. Many Entity Framework examples detail adding Entity Framework support to an existing component (resource, like console app, ASP.NET Core web API, Razor app, etc.), creating a circular dependency between its project and the existence of an executing database (i.e., a valid database connection string.) In database-first, you have an existing application with existing physical databases and practices to utilize them in a development environment. With .NET Aspire, developers are &lt;em&gt;shifted left&lt;/em&gt; from the decisions that provide the resources that things like &lt;code&gt;migrations add &amp;lt;migration-name&amp;gt;&lt;/code&gt; and &lt;code&gt;dotnet ef database update&lt;/code&gt; require to function properly.&lt;/p&gt;
&lt;p&gt;To be clear, the way .NET Aspire works is that the orchestration (AppHost) executes, figures out the various connection strings, and overrides the &lt;code&gt;appsettings&lt;/code&gt; by setting environment variables before running the other components. The premise behind this means that at run-time, whatever is in &lt;code&gt;appsettings&lt;/code&gt; is ignored. &lt;code&gt;dotnet ef&lt;/code&gt; command doesn't execute at run-time; it effectively runs at design-time and gets its configuration from &lt;code&gt;appsettings&lt;/code&gt;, so it's out of sync with reality.&lt;/p&gt;
&lt;p&gt;The basic guidance is to &lt;em&gt;abstract those types of dependencies as .NET Aspire resources&lt;/em&gt;. Nothing new conceptually, but this might be an application of the principles of abstraction at a level less commonly applied. Refining that guidance to using Entity Framework: &lt;em&gt;the database should be an independent resource&lt;/em&gt;. Independent resources are modeled in .NET as either separate projects or separate solutions. Luckily, an &lt;a href="https://github.com/dotnet/aspire-samples/tree/main/samples/eShopLite"&gt;.NET Aspire sample&lt;/a&gt; addresses this. Let's look into the details.&lt;/p&gt;
&lt;p&gt;The structure of the eShopLite sample overlaps with the .NET Aspire starter &lt;code&gt;dotnet new&lt;/code&gt; template. It has a Blazor web frontend, a web API, an Aspire AppHost, and an Aspire service defaults project. Additionally, there is a shopping cart service (BasketService), and the catalog database (CatalogDb) project is an abstraction of the database resource.&lt;/p&gt;
&lt;p&gt;The CatalogDb looks very similar to what you'd end up with following &lt;a href="https://learn.microsoft.com/en-us/aspnet/core/tutorials/first-web-api?view=aspnetcore-8.0&amp;amp;tabs=visual-studio"&gt;Tutorial: Create a web API with ASP.NET Core&lt;/a&gt;: an ASP.NET Core web API that leverages Entity Framework, and is effectively a gateway to a backend database. Although, that tutorial uses Entity Framework &lt;em&gt;in-memory&lt;/em&gt; rather than via PostgreSQL. The way eShopLite supports Entity Framework is through the CatalogDb project. CatalogDb is like a stub project to the rest of the solution: Aspire doesn't execute it, but CatalogService depends upon it for the database model classes and &lt;code&gt;DbContext&lt;/code&gt; (utilized more like a class library.)  Nothing connects to the CatalogDb &lt;em&gt;web API&lt;/em&gt;. The CatalogDb project contains all the Entity Framework design-time details and references, allowing you to utilize Entity Framework's features like &lt;code&gt;migrations add &amp;lt;migration-name&amp;gt;&lt;/code&gt; and &lt;code&gt;dotnet ef database update&lt;/code&gt;. The target of Entity Framework operations like migration add and database update would depend on the configuration in &lt;code&gt;appsettings.json&lt;/code&gt;. Initialization/seeding of the data is handled in &lt;code&gt;CatalogDbInitializer&lt;/code&gt; within CatalogDb, as well as migrations at run-time (startup). CatalogDb &lt;code&gt;appsettings&lt;/code&gt; connection strings must be in sync with the run-time values for &lt;code&gt;ef&lt;/code&gt; commands to work.&lt;/p&gt;
&lt;p&gt;In summary, if you want to utilize Entity Framework in a basic .NET Aspire application, &lt;a href="https://learn.microsoft.com/en-us/aspnet/core/tutorials/first-web-api?view=aspnetcore-8.0&amp;amp;tabs=visual-studio"&gt;adding a project&lt;/a&gt; to contain the entity models, context, and Entity Framework references and supporting a database engine container is a recommended place to get started. I suspect this guidance may be refined as .NET Aspire evolves.&lt;/p&gt;
&lt;p&gt;I'm still wrapping my head around how .NET Aspire can support other non-containerized workloads like Azure SQL. Still, a containerized design melds nicely with the idea of independent resources (or nodes) in .NET Aspire.  .NET Aspire also helps to more clearly delineate concerns like design, build, test, release, and deploy. As with .NET Aspire, containerization is an easier starting point for someone interested in distributed applications.&lt;/p&gt;
&lt;p&gt;I look forward to how .NET Aspire evolves.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/entity-framework-in-aspire.jpg" class="img-fluid" alt="A path through the infrastructure"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/etags-in-aspdotnet-core</id>
		<title>ETags in ASP.NET Core</title>
		<link href="http://blog.peterritchie.com/posts/etags-in-aspdotnet-core" />
		<updated>2023-06-28T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="../assets/farside-etags-in-asp-dot-net.jpg" class="img-fluid" alt="lots of things going on at the same time, in the style of Farside" /&gt;&lt;/p&gt;
&lt;p&gt;A good software architect doesn't just provide expectations of structure; they also work with developers to give feedback and guidance for implementation. It's easy to say, &amp;quot;Use ETags for entity concurrency control in a Web API,&amp;quot; it's another to empower teams to accomplish the objectives of entity versioning.&lt;/p&gt;
&lt;p&gt;To review: entity-tags (Etags) are a method of implementing Optimistic Concurrency Control. Optimistic Concurrency Control is a means to avoid distributed locking in situations when two or more potentially concurrent operations rarely interfere with each other. You can see cases like this on the Web when multiple processes or people are not normally working on the same data simultaneously. With the Web, there are rare situations where a single process or single person can (usually inadvertently) modify data from two places at the same time. It's rare case like this where the low overhead of optimistic concurrency can avoid accidental overwrites.&lt;/p&gt;
&lt;p&gt;Entity-tags are a moniker of a particular incarnation of an entity. The tag is opaque, so it shouldn't need to be interpretable by a requestor to your service. With opaque data, you want to make the value itself as unobvious as possible.&lt;/p&gt;
&lt;p&gt;The value, of course, could be an incrementing integer if you could reliably and efficiently increment an integer in a distributed environment (remember, we're addressing the possibility of two distributed transactions interfering with one another, the same transactions mechanisms that would be used to increment an integer.)  But, before choosing to increment an integer (an ordinal number), consider &lt;a href="https://www.rfc-editor.org/rfc/rfc9110#field.etag"&gt;RFC 9110 ETags&lt;/a&gt; and why ordinal version numbers are not specified.&lt;/p&gt;
&lt;p&gt;| If you think an ordinal number will work, do you need entity-tags at all?&lt;/p&gt;
&lt;p&gt;A time-stamp is something to consider, in which case, prefer the &lt;code&gt;Last-Modified&lt;/code&gt; header field validator. Or in conjunction with entity-tags. If a time-stamp is reliable, &lt;code&gt;Last-Modified&lt;/code&gt; offers better interoperability options than re-inventing the wheel. Also, be thoughtful when considering time-stamps, especially their granularity; per-second time-stamp granularity can only partially solve the problem of concurrent writes.&lt;/p&gt;
&lt;p&gt;So, how do you reliably generate an entity-tag value? The first thing to consider is what you want to accomplish. Do you want to prevent accidental overwrites, or do you want entity versioning? If you said, &amp;quot;I want entity versioning,&amp;quot; to what end? If a client gets version 1, and another client updates it to version 2, what action do you want to perform when the first client requests to update the entity? You don't need &lt;em&gt;versioning&lt;/em&gt; to prevent that first client from updating the entity. If you want to merge with version 2, you probably want versioning; in this case, you can stop reading now; I won't get into detail like that in this post.&lt;/p&gt;
&lt;p&gt;If we're interested in preventing accidental overwrites, on the server side, we only really care about the current entity and the basis for the current request to update it. It doesn't matter if the basis is the previous version or ten versions behind; we only care that it's not based on the current version.&lt;/p&gt;
&lt;p&gt;Another thing to consider is that entity-tags are also used in HTTP caching, which requires that an entity-tag be unique per encoding (e.g., a gzipped response should have a different entity-tag than a non-gzipped response.) The encoding value is often postfixed to the entity-tag to make it unique per encoding. But be careful to parse that out when checking for semantically identical entities. That's out of the scope of this post.&lt;/p&gt;
&lt;p&gt;With an understanding of those constraints, a common method of generating an entity-tag is to use a hash of the entity representation.&lt;/p&gt;
&lt;p&gt;Let's look at an example controller that tries to isolate the implementation detail of how the entity-tag is calculated. For my examples, I'm choosing to use controllers over minimal APIs; the controller class attributes make some of what is required easier. For clarity, my examples are stripped of error responses unrelated to conditional requests and exception middleware. For complete source, see &lt;a href="https://github.com/peteraritchie/Examples.Etag"&gt;this repo&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[ApiController] [Route(&amp;quot;[controller]&amp;quot;)]
public class AppointmentController : ControllerBase
{
	[HttpGet(Name = &amp;quot;GetAppointmentRequests&amp;quot;)]
	[ProducesResponseType(typeof(WebCollectionElement&amp;lt;AppointmentRequestDto&amp;gt;[]),
		StatusCodes.Status200OK, MediaTypeNames.Application.Json)]
	public async Task&amp;lt;IActionResult&amp;gt; GetMany(CancellationToken cancellationToken = default)
	{
		var resource = appointmentRequestService.GetRequests(cancellationToken);

		List&amp;lt;WebCollectionElement&amp;lt;AppointmentRequestDto&amp;gt;&amp;gt; items = new();
		foreach (var (dto, guid, concurrencyToken) in await resource.ToListAsync(cancellationToken: cancellationToken))
		{
			items.Add(
				new WebCollectionElement&amp;lt;AppointmentRequestDto&amp;gt;(dto, Url.Action(nameof(GetById),
					new { id = guid })!, etag: concurrencyToken));
		}

		return base.Ok(items);
	}

	[HttpGet(&amp;quot;{id}&amp;quot;, Name = &amp;quot;GetAppointmentRequest&amp;quot;)]
	[ProducesResponseType(typeof(AppointmentRequestDto), StatusCodes.Status200OK, MediaTypeNames.Application.Json)]
	[ProducesResponseType(typeof(AppointmentRequestDto), StatusCodes.Status304NotModified)]
	public async Task&amp;lt;IActionResult&amp;gt; GetById(Guid id, [FromHeader(Name = &amp;quot;If-None-Match&amp;quot;)] string? ifNoneMatch,
		CancellationToken cancellationToken = default)
	{
		var (resource, concurrencyToken) = string.IsNullOrWhiteSpace(ifNoneMatch) 
			? await appointmentRequestService.GetRequest(id, cancellationToken) 
			: await appointmentRequestService.GetRequest(id, ifNoneMatch, cancellationToken);

		HttpContext.Response.Headers.Add(HeaderNames.ETag, concurrencyToken);

		return Ok(resource);
	}

	[HttpPost(Name = &amp;quot;CreateAppointmentRequest&amp;quot;)]
	[Consumes(MediaTypeNames.Application.Json)]
	[ProducesResponseType(StatusCodes.Status201Created)]
	public async Task&amp;lt;IActionResult&amp;gt; Create([FromBody] AppointmentRequestDto appointmentRequest,
		CancellationToken cancellationToken = default)
	{
		var (id, concurrencyToken) = await appointmentRequestService.CreateRequest(appointmentRequest, cancellationToken);

		HttpContext.Response.Headers.Add(HeaderNames.ETag, concurrencyToken);

		return CreatedAtAction(nameof(GetById), routeValues: new { id }, value: null);
	}

	[HttpPut(&amp;quot;{id}&amp;quot;, Name = &amp;quot;ReplaceAppointmentRequest&amp;quot;)]
	[Consumes(MediaTypeNames.Application.Json)]
	[ProducesResponseType(StatusCodes.Status204NoContent)]
	public async Task&amp;lt;IActionResult&amp;gt; Replace(Guid id, [FromBody] AppointmentRequestDto appointmentRequest,
		[FromHeader(Name = &amp;quot;If-Match&amp;quot;)] string? ifMatch, CancellationToken cancellationToken = default)
	{
		var concurrencyToken = string.IsNullOrWhiteSpace(ifMatch)
			? await appointmentRequestService.ReplaceRequest(
				id, appointmentRequest, cancellationToken)
			: await appointmentRequestService.ReplaceRequest(
				id, appointmentRequest, ifMatch, cancellationToken);

		HttpContext.Response.Headers.Add(HeaderNames.ETag, concurrencyToken);

		return NoContent();
	}

	[HttpPatch(&amp;quot;{id:guid}&amp;quot;, Name = &amp;quot;UpdateAppointmentRequest&amp;quot;)]
	[ProducesResponseType(typeof(AppointmentRequestDto), StatusCodes.Status200OK, MediaTypeNames.Application.Json)]
	[Consumes(&amp;quot;application/json-patch+json&amp;quot;)]
	public async Task&amp;lt;IActionResult&amp;gt; Update(Guid id, JsonPatchDocument&amp;lt;AppointmentRequestDto&amp;gt; patchDocument,
		[FromHeader(Name = &amp;quot;If-Match&amp;quot;)] string? ifMatch, CancellationToken cancellationToken = default)
	{
		var (result, concurrencyToken) = string.IsNullOrWhiteSpace(ifMatch)
			? await appointmentRequestService.UpdateRequest(id, patchDocument, cancellationToken)
			: await appointmentRequestService.UpdateRequest(id, patchDocument, ifMatch, cancellationToken);

		HttpContext.Response.Headers.Add(HeaderNames.ETag, concurrencyToken);

		return Ok(result);
	}

	[HttpDelete(&amp;quot;{id}&amp;quot;, Name = &amp;quot;RemoveAppointmentRequest&amp;quot;)]
	[ProducesResponseType(StatusCodes.Status204NoContent)]
	public async Task&amp;lt;IActionResult&amp;gt; Remove(Guid id, [FromHeader(Name = &amp;quot;If-Match&amp;quot;)] string? ifMatch,
		CancellationToken cancellationToken = default)
	{
		if(string.IsNullOrWhiteSpace(ifMatch))
			await appointmentRequestService.RemoveRequest(id, cancellationToken);
		else
			await appointmentRequestService.RemoveRequest(id, ifMatch, cancellationToken);

		return NoContent();
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There are inherent complexities in a Web API. It needs to present an interface usable on the Web and utilizes open standards as much as possible. You'll notice that the &lt;code&gt;AppointmentController&lt;/code&gt; PATCH implementation uses &lt;code&gt;JsonPatchDocument&lt;/code&gt;, an implementation of the &lt;a href="https://jsonpatch.com/"&gt;JSON Patch&lt;/a&gt; (IETF RFC 6902) standard. This standard is specific to the Web, specific to JSON, and deals with operations intended to be specifically applied to JSON representations equivalent to the model defined in the interface (i.e., the model, not what is represented in the database or an in-memory representation of a domain object.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This controller is isolated from the collaboration with the database and delegates that interaction to an Application Service via the &lt;code&gt;appointmentRequestService&lt;/code&gt; field (declaration removed for readability). In state-modifying HTTP methods (PUT, DELETE, PATCH), the actions have an &lt;code&gt;ifMatch&lt;/code&gt; parameter passed in through the &lt;code&gt;If-Match&lt;/code&gt; HTTP request header. When present, it is passed along to the application service for optimistic concurrency. This example shows an &lt;em&gt;optional&lt;/em&gt; use of &lt;code&gt;If-Match&lt;/code&gt;; it's plausible that another implementation might &lt;em&gt;require&lt;/em&gt; the &lt;code&gt;If-Match&lt;/code&gt; header and respond with status code 428 Precondition Required.&lt;/p&gt;
&lt;p&gt;Of note is that this controller abstracts etag header values as &lt;em&gt;concurrency token&lt;/em&gt; text so that nothing else has to deal with HTTP headers.&lt;/p&gt;
&lt;p&gt;Let's look at the MVC model (I prefer to refer to it as a Data Transfer Object).&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class AppointmentRequestDto
{
	[Required]
	public DateTime? CreationDate { get; set; }
	public IEnumerable&amp;lt;string&amp;gt;? Categories { get; set; }
	[Required]
	public string? Description { get; set; }
	public string? Notes { get; set; }
	[Required]
	public AppointmentRequestStatus? Status { get; set; }
	[Required]
	public MeetingDuration? Duration { get; set; }
	[Required]
	public IEnumerable&amp;lt;string&amp;gt;? Participants { get; set; }
	[Required]
	public IEnumerable&amp;lt;DateTime&amp;gt;? ProposedStartDateTimes { get; set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we're delegating serialization to ASP.NET (which requires writable properties), the properties are nullable but annotated with &lt;code&gt;RequiredAttribute&lt;/code&gt; to signal to the framework what properties are required. There is no identifier in the &lt;code&gt;AppointmentRequestDto&lt;/code&gt; class because we don't want to duplicate it there and in the resource's URI.&lt;/p&gt;
&lt;p&gt;Azure Cosmos has implemented optimistic concurrency control and stores an ETag per document. I'll use Azure Cosmos for the database implementation to show how this can be re-used in your WebAPI.&lt;/p&gt;
&lt;h2 id="azure-cosmos-example"&gt;Azure Cosmos Example&lt;/h2&gt;
&lt;p&gt;In Azure Cosmos, each document has several mandatory properties: &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;_rid&lt;/code&gt;, &lt;code&gt;_self&lt;/code&gt;, &lt;code&gt;_etag&lt;/code&gt;, &lt;code&gt;_attachements&lt;/code&gt;, and &lt;code&gt;_ts&lt;/code&gt;. These are implementation details of the database that we don't want to leak into our API as body content. When we use the &lt;a href="https://www.nuget.org/packages/Microsoft.Azure.Cosmos"&gt;Azure Cosmos SDK&lt;/a&gt;, we need serialization classes to serialize the data to and from a container. Let's see an example with a fictitious appointment request resource:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class AppointmentRequestEntity : CosmosEntityBase
{
    [JsonProperty(PropertyName = &amp;quot;id&amp;quot;)]
    public Guid Id { get; set; }
    [JsonProperty(PropertyName = &amp;quot;_rid&amp;quot;)]
    public string? ResourceId { get; set; }
    [JsonProperty(PropertyName = &amp;quot;_self&amp;quot;)]
    public Uri? SelfUri { get; set; }
    [JsonProperty(PropertyName = &amp;quot;_etag&amp;quot;)]
    public string? ETag{ get; set; }
    [JsonProperty(PropertyName = &amp;quot;_ts&amp;quot;)]
    public int? TimestampText{ get; set; }
	public DateTime? CreationDate { get;  set; }
	public IEnumerable&amp;lt;string&amp;gt;? Categories { get; set; }
	public string? Description { get;  set; }
	public string? Notes { get;  set;  }
	public AppointmentRequestStatus? Status { get;  set; }
	public MeetingDuration? Duration { get;  set; }
	public IEnumerable&amp;lt;string&amp;gt;? Participants { get; set; }
	public IEnumerable&amp;lt;DateTime&amp;gt;? ProposedStartDateTimes { get;  set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the first five properties that are necessary to access the Azure Cosmos implementation details. (in &lt;a href="https://github.com/peteraritchie/Examples.Etag"&gt;this repo&lt;/a&gt; this is split out into a &lt;code&gt;CosmosEntityBase&lt;/code&gt; class.)&lt;/p&gt;
&lt;p&gt;For my example, I'm going to draw on Domain-Driven design patterns and use a Repository implementation in the database collaboration. I want to delegate all the logic related to database-specific details to the repository implementation. This includes encapsulating the use of the database entity &lt;em&gt;serialization&lt;/em&gt; class (translation to/from the database entity class), associating an identifier and etag with the resource, etc. To separate the existence of the database entity class from clients of the repository, we'll define a generic interface that I'll name &lt;code&gt;IOptimisticallyConcurrentRepository&lt;/code&gt; that works with different types of domain entity classes:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public interface IOptimisticallyConcurrentRepository&amp;lt;TDomainEntity&amp;gt;
{
    Task&amp;lt;TDomainEntity&amp;gt; Get(Guid id, CancellationToken cancellationToken = default);
    IAsyncEnumerable&amp;lt;TDomainEntity&amp;gt; Get(CancellationToken cancellationToken = default);
	Guid GetId(TDomainEntity entity);

	bool TryGetIfModified(Guid id, string concurrencyToken, out TDomainEntity? entity);
	string GetConcurrencyToken(TDomainEntity entity);

    Task&amp;lt;Guid&amp;gt; Add(TDomainEntity entity, CancellationToken cancellationToken = default);
    Task Remove(Guid id, CancellationToken cancellationToken = default);
    Task Replace(Guid id, TDomainEntity entity, CancellationToken cancellationToken = default);

    Task RemoveIfMatch(Guid id, string token, CancellationToken cancellationToken = default);
    Task ReplaceIfMatch(Guid id, TDomainEntity entity, string token, CancellationToken cancellationToken = default);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next is a generic repository class to support Azure Cosmos that deals with arbitrary domain (&lt;code&gt;TDomainEntity&lt;/code&gt;) and database serialization classes (&lt;code&gt;TDbEntity&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class CosmosOptimisticallyConcurrentRepository&amp;lt;TDomainEntity, TDbEntity&amp;gt; 
	: IOptimisticallyConcurrentRepository&amp;lt;TDomainEntity&amp;gt;
	where TDomainEntity : class
	where TDbEntity : CosmosEntityBase
{
	private class EntityContext
	{
		public EntityContext(Guid id, string concurrencyToken)
		{
			Id = id;
			ConcurrencyToken = concurrencyToken;
		}

		public Guid Id { get; }
		public string ConcurrencyToken { get; }
	}

	private readonly Container container;
	private readonly ITranslator&amp;lt;TDomainEntity, TDbEntity&amp;gt; dbEntityTranslator;
	private readonly Action&amp;lt;TDbEntity, Guid&amp;gt; setDbEntityId;

	protected CosmosOptimisticallyConcurrentRepository(Container container, ITranslator&amp;lt;TDomainEntity, TDbEntity&amp;gt; dbEntityTranslator,
		Action&amp;lt;TDbEntity, Guid&amp;gt; setDbEntityId)
	{
		this.container = container;
		this.dbEntityTranslator = dbEntityTranslator;
		this.setDbEntityId = setDbEntityId;
	}

	public async Task&amp;lt;Guid&amp;gt; Add(TDomainEntity entity, CancellationToken cancellationToken = default)
	{
		var id = Guid.NewGuid();
		var dbEntity = dbEntityTranslator.ToData(entity);
		setDbEntityId(dbEntity, id);

		try
		{
			var result = await container.CreateItemAsync(dbEntity, new PartitionKey(id.ToString(&amp;quot;D&amp;quot;)), cancellationToken: cancellationToken);
			conditionalWeakTable.Add(entity, new EntityContext(id, result.ETag));
			return id;
		}
		catch (CosmosException ex) when(ex.StatusCode == HttpStatusCode.PreconditionFailed)
		{
			throw new ConcurrencyException();
		}
	}

	public bool TryGetIfModified(Guid id, string concurrencyToken, out TDomainEntity? entity)
	{
		var idText = id.ToString(&amp;quot;D&amp;quot;);
		try
		{
			var result = container.ReadItemAsync&amp;lt;TDbEntity&amp;gt;(
					idText,
					new PartitionKey(idText),
					requestOptions: new ItemRequestOptions() { IfNoneMatchEtag = concurrencyToken })
				.Result;

			entity = dbEntityTranslator.ToDomain(result.Resource);
			conditionalWeakTable.Add(entity, new EntityContext(id, result.ETag));
			return true;
		}
		catch (AggregateException aggregateException) when (aggregateException.InnerExceptions.Count == 1 &amp;amp;&amp;amp;
		                                                    aggregateException.InnerExceptions.Single() is
			                                                    CosmosException
			                                                    {
				                                                    StatusCode: HttpStatusCode.NotModified
			                                                    })
		{
			entity = default;
			return false;
		}
		catch (AggregateException aggregateException) when (aggregateException.InnerExceptions.Count == 1 &amp;amp;&amp;amp;
		                                                    aggregateException.InnerExceptions.Single() is
			                                                    CosmosException
			                                                    {
				                                                    StatusCode: HttpStatusCode.NotFound
			                                                    })
		{
			throw new EntityNotFoundException(id);
		}
		catch (CosmosException ex) when(ex.StatusCode == HttpStatusCode.NotFound)
		{
			throw new EntityNotFoundException(id);
		}
	}

	public async IAsyncEnumerable&amp;lt;TDomainEntity&amp;gt; Get([EnumeratorCancellation] CancellationToken cancellationToken = default)
	{
		var iterator = container.GetItemQueryIterator&amp;lt;TDbEntity&amp;gt;();
		while (iterator.HasMoreResults)
		{
			var set = await iterator.ReadNextAsync(cancellationToken);
			foreach (var e in set)
			{
				var entity = dbEntityTranslator.ToDomain(e);
				conditionalWeakTable.Add(entity, new EntityContext(e.Id, e.ETag!));
				yield return entity;
			}
		}
	}

	public async Task&amp;lt;TDomainEntity&amp;gt; Get(Guid id, CancellationToken cancellationToken = default)
	{
		var idText = id.ToString(&amp;quot;D&amp;quot;);
		try
		{
			var result = await container.ReadItemAsync&amp;lt;TDbEntity&amp;gt;(idText, new PartitionKey(idText), cancellationToken: cancellationToken);
			var entity = dbEntityTranslator.ToDomain(result.Resource);
			conditionalWeakTable.Add(entity, new EntityContext(id, result.ETag));
			return entity;
		}
		catch (CosmosException ex) when(ex.StatusCode == HttpStatusCode.NotFound)
		{
			throw new EntityNotFoundException(id);
		}
	}

	public async Task Replace(Guid id, TDomainEntity entity, CancellationToken cancellationToken = default)
	{
		var dbEntity = dbEntityTranslator.ToData(entity);
		setDbEntityId(dbEntity, id);

		try
		{
			_ = await container.UpsertItemAsync(dbEntity, cancellationToken: cancellationToken);
		}
		catch (CosmosException ex) when(ex.StatusCode == HttpStatusCode.NotFound)
		{
			throw new EntityNotFoundException(id);
		}
	}

	public async Task ReplaceIfMatch(Guid id, TDomainEntity entity, string token, CancellationToken cancellationToken = default)
	{
		var idText = id.ToString(&amp;quot;D&amp;quot;);
		var dbEntity = dbEntityTranslator.ToData(entity);
		setDbEntityId(dbEntity, id);

		var requestOptions = new ItemRequestOptions { IfMatchEtag = token };
		try
		{
			_ = await container.ReplaceItemAsync(dbEntity, idText, new PartitionKey(idText), requestOptions: requestOptions, cancellationToken: cancellationToken);
		}
		catch (CosmosException ex) when (ex.StatusCode == HttpStatusCode.PreconditionFailed)
		{
			throw new ConcurrencyException();
		}
		catch (CosmosException ex) when(ex.StatusCode == HttpStatusCode.NotFound)
		{
			throw new EntityNotFoundException(id);
		}
	}

	public async Task Remove(Guid id, CancellationToken cancellationToken = default)
	{
		var idText = id.ToString(&amp;quot;D&amp;quot;);

		try
		{
			_ = await container.DeleteItemAsync&amp;lt;TDbEntity&amp;gt;(idText, new PartitionKey(idText), cancellationToken: cancellationToken);
		}
		catch (CosmosException ex) when(ex.StatusCode == HttpStatusCode.NotFound)
		{
			throw new EntityNotFoundException(id);
		}
	}

	public async Task RemoveIfMatch(Guid id, string token, CancellationToken cancellationToken = default)
	{
		var idText = id.ToString(&amp;quot;D&amp;quot;);

		var requestOptions = new ItemRequestOptions { IfMatchEtag = token };
		try
		{
			_ = await container.DeleteItemAsync&amp;lt;TDbEntity&amp;gt;(idText, new PartitionKey(idText), requestOptions: requestOptions, cancellationToken: cancellationToken);
		}
		catch (CosmosException ex) when (ex.StatusCode == HttpStatusCode.PreconditionFailed)
		{
			throw new ConcurrencyException();
		}
		catch (CosmosException ex) when(ex.StatusCode == HttpStatusCode.NotFound)
		{
			throw new EntityNotFoundException(id);
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a reminder, a &amp;quot;concurrency token&amp;quot; is synonymous with an &amp;quot;etag&amp;quot; in the context of the repository.&lt;/p&gt;
&lt;p&gt;The persistence needs of an application are independent of a domain entity, so the domain entity is isolated from web/database identifiers, concurrency tokens, HTTP, etags, etc. So, the repository needs to translate from a domain object to the serialization object, which is performed mostly by an &lt;code&gt;ITranslator&amp;lt;TDomain, TData&amp;gt;&lt;/code&gt; implementation but also with the assignment of the identifier to the serialization object. To keep the non-domain details isolated from the domain object, I've used the &lt;code&gt;ConditionalWeakTable&amp;lt;TKey, TValue&amp;gt;&lt;/code&gt; type to associate database persistence details (ID and etag/concurrency token, as abstracted by &lt;code&gt;EntityContext&lt;/code&gt;) to the object without too much management logic.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;code&gt;ConditionalWeakTable&lt;/code&gt; is like a dictionary that associates a value with another object. It differs from a traditional dictionary in that when the key is no longer referenced, the associated value is freed/destroyed. This allows us to get associated data with minimal memory impact easily.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An implementation of the repository now just requires the type to use for the database serialization class, the domain entity type, and how to assign an identifier to the Azure Cosmos &lt;code&gt;id&lt;/code&gt; property:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public sealed class CosmosAppointmentRequestRepository : CosmosOptimisticallyConcurrentRepository&amp;lt;AppointmentRequest, AppointmentRequestEntity&amp;gt;
{
    public CosmosAppointmentRequestRepository(Container container, ITranslator&amp;lt;AppointmentRequest, AppointmentRequestEntity&amp;gt; appointmentRequestEntityTranslator)
        : base(container, appointmentRequestEntityTranslator, (entity, guid) =&amp;gt; entity.Id = guid)
    {
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only remaining part is the implementation of the application/database collaboration, the &lt;em&gt;application service&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class AppointmentRequestService
{
	private readonly AppointmentRequestDtoTranslator appointmentRequestDtoTranslator;
	private readonly IOptimisticallyConcurrentRepository&amp;lt;AppointmentRequest&amp;gt; repository;

	public AppointmentRequestService(AppointmentRequestDtoTranslator appointmentRequestDtoTranslator, IOptimisticallyConcurrentRepository&amp;lt;AppointmentRequest&amp;gt; repository)
	{
		this.appointmentRequestDtoTranslator = appointmentRequestDtoTranslator;
		this.repository = repository;
	}

	public async Task&amp;lt;(Guid, string)&amp;gt; CreateRequest(AppointmentRequestDto appointmentRequest, CancellationToken cancellationToken = default)
	{
		var entity = appointmentRequestDtoTranslator.AppointmentRequestDtoToAppointmentRequest(appointmentRequest);
		var guid = await repository.Add(entity, cancellationToken);

		return (guid, repository.GetConcurrencyToken(entity));
	}

	public async Task&amp;lt;(AppointmentRequestDto, string)&amp;gt; GetRequest(Guid id, CancellationToken cancellationToken = default)
	{
		var appointmentRequest = await repository.Get(id, cancellationToken);
		return (appointmentRequestDtoTranslator.AppointmentRequestToAppointmentRequestDto(appointmentRequest), repository.GetConcurrencyToken(appointmentRequest));
	}

	public Task&amp;lt;(AppointmentRequestDto, string)&amp;gt; GetRequest(Guid id, string etag, CancellationToken _ = default)
	{
		if(repository.TryGetIfModified(id, etag, out var appointmentRequest))
		{ 
			return Task.FromResult((appointmentRequestDtoTranslator.AppointmentRequestToAppointmentRequestDto(appointmentRequest!), repository.GetConcurrencyToken(appointmentRequest!)));
		}

		throw new ConcurrencyException();
	}

	public async IAsyncEnumerable&amp;lt;(AppointmentRequestDto, Guid, string)&amp;gt; GetRequests([EnumeratorCancellation] CancellationToken cancellationToken = default)
	{
		var result = repository.Get(cancellationToken);
		await foreach (var item in result.WithCancellation(cancellationToken))
		{
			yield return (appointmentRequestDtoTranslator.AppointmentRequestToAppointmentRequestDto(item), repository.GetId(item),
				repository.GetConcurrencyToken(item));
		}
	}

	public async Task RemoveRequest(Guid id, CancellationToken cancellationToken = default)
	{
		await repository.Remove(id, cancellationToken);
	}

	public async Task RemoveRequest(Guid id, string etag, CancellationToken cancellationToken = default)
	{
		await repository.RemoveIfMatch(id, etag, cancellationToken);
	}

	internal async Task&amp;lt;string&amp;gt; ReplaceRequest(Guid id, AppointmentRequestDto appointmentRequest,
		CancellationToken cancellationToken = default)
	{
		var entity = appointmentRequestDtoTranslator.AppointmentRequestDtoToAppointmentRequest(appointmentRequest);
		await repository.Replace(id, entity, cancellationToken);

		return repository.GetConcurrencyToken(entity);
	}

	internal async Task&amp;lt;string&amp;gt; ReplaceRequest(Guid id, AppointmentRequestDto appointmentRequest, string etag,
		CancellationToken cancellationToken = default)
	{
		var entity = appointmentRequestDtoTranslator.AppointmentRequestDtoToAppointmentRequest(appointmentRequest);
		await repository.ReplaceIfMatch(id, entity, etag, cancellationToken);

		return repository.GetConcurrencyToken(entity);
	}

	public async Task&amp;lt;(AppointmentRequestDto, string)&amp;gt; UpdateRequest(Guid id, JsonPatchDocument&amp;lt;AppointmentRequestDto&amp;gt; patchDocument,
		CancellationToken cancellationToken = default)
	{
		var current = await repository.Get(id, cancellationToken);
		var currentDto = appointmentRequestDtoTranslator.AppointmentRequestToAppointmentRequestDto(current);
		patchDocument.ApplyTo(currentDto);
		await repository.Replace(id, appointmentRequestDtoTranslator.AppointmentRequestDtoToAppointmentRequest(currentDto), cancellationToken);
		return (currentDto, repository.GetConcurrencyToken(current));
	}

	public async Task&amp;lt;(AppointmentRequestDto, string)&amp;gt; UpdateRequest(Guid id, JsonPatchDocument&amp;lt;AppointmentRequestDto&amp;gt; patchDocument,
		string etag, CancellationToken cancellationToken = default)
	{
		var current = await repository.Get(id, cancellationToken);
		var currentDto = appointmentRequestDtoTranslator.AppointmentRequestToAppointmentRequestDto(current);
		patchDocument.ApplyTo(currentDto);
		await repository.ReplaceIfMatch(id, appointmentRequestDtoTranslator.AppointmentRequestDtoToAppointmentRequest(currentDto), etag, cancellationToken);
		return (currentDto, repository.GetConcurrencyToken(current));
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;AppointmentRequestService&lt;/code&gt; contains the interaction logic specific to the application and the repository. Since there&lt;/p&gt;
&lt;p&gt;Dealing with translation to and from DTO, domain, and serialization classes is made less of a chore with tools like &lt;a href="https://github.com/riok/mapperly"&gt;Mapperly&lt;/a&gt;. &lt;a href="https://github.com/riok/mapperly"&gt;Mapperly&lt;/a&gt; will generate translation code based on property names. To create a translator to/from two types is easy as creating a partial class with a &lt;code&gt;MapperAttribute&lt;/code&gt; attribute with partial methods that take one type as parameter and the other as a return:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[Mapper]
public partial class AppointmentRequestDtoTranslator
{
	public partial AppointmentRequest AppointmentRequestDtoToAppointmentRequest(AppointmentRequestDto dto);
	public partial AppointmentRequestDto AppointmentRequestToAppointmentRequestDto(AppointmentRequest entity);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;AppointmentRequestDtoTranslator&lt;/code&gt; translates &lt;code&gt;AppointmentRequestDto&lt;/code&gt; instances to/from &lt;code&gt;AppointmentRequest&lt;/code&gt; domain entity instances. And to translate to/from &lt;code&gt;AppointmentRequestEntity&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[Mapper]
public partial class AppointmentRequestEntityTranslator : ITranslator&amp;lt;AppointmentRequest, AppointmentRequestEntity&amp;gt;
{
	[MapperIgnoreSource(nameof(AppointmentRequestEntity.Id))]
	[MapperIgnoreSource(nameof(AppointmentRequestEntity.ResourceId))]
	[MapperIgnoreSource(nameof(AppointmentRequestEntity.ETag))]
	[MapperIgnoreSource(nameof(AppointmentRequestEntity.SelfUri))]
	[MapperIgnoreSource(nameof(AppointmentRequestEntity.TimestampText))]
	public partial AppointmentRequest ToDomain(AppointmentRequestEntity data);

	[MapperIgnoreTarget(nameof(AppointmentRequestEntity.Id))]
	[MapperIgnoreTarget(nameof(AppointmentRequestEntity.ResourceId))]
	[MapperIgnoreTarget(nameof(AppointmentRequestEntity.ETag))]
	[MapperIgnoreTarget(nameof(AppointmentRequestEntity.SelfUri))]
	[MapperIgnoreTarget(nameof(AppointmentRequestEntity.TimestampText))]
	public partial AppointmentRequestEntity ToData(AppointmentRequest domain);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;AppointmentRequestEntity&lt;/code&gt; has some Azure Cosmos implementation details, we use Mapprerly's &lt;code&gt;MapperIgnoreTargetAttribute&lt;/code&gt; and &lt;code&gt;MapperIgnoreSourceAttribute&lt;/code&gt; to tell &lt;a href="https://github.com/riok/mapperly"&gt;Mapperly&lt;/a&gt; that not all properties need translation.&lt;/p&gt;
&lt;p&gt;Dealing with concurrency issues and implementing concurrency control can be intimidating. In this post, I make it less intimidating by clarifying some specifics by showing an example implementation with ASP.NET Core and Azure Cosmos DB. Additionally, the Domain-Driven Design patterns Repository and Application Service are used to isolate etag implementation details from the Web API to delegate that to Azure Cosmos.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There are multiple ways of implementing optimistic concurrency; HTTP ETags are but one way. If you can't abide by the expectations set out by the HTTP standards, don't use Etags. There's nothing that forces you to use HTTP precondition header fields. But, remember, the means exist in HTTP, and embracing it will promote interoperability and reliability (to implement something different than something introduced at least 26 years ago fails to recognize the huge amount of validation and verification that's gone into making it correct.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a future post, I will show an example of a repository implementation that uses Entity Framework and its expectations for concurrency tokens.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/farside-etags-in-asp-dot-net.jpg" class="img-fluid" alt="lots of things going on at the same time, in the style of Farside"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/http-and-etag-header-fields</id>
		<title>HTTP and ETag Header Fields</title>
		<link href="http://blog.peterritchie.com/posts/http-and-etag-header-fields" />
		<updated>2023-06-15T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="../assets/accidental-overwrite.jpg" class="img-fluid" alt="A stuffed tiger corrupted appearance due to accidental overwrite" /&gt;&lt;/p&gt;
&lt;p&gt;Update: corrected mention of &lt;code&gt;412&lt;/code&gt; in the context of &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;If-Modified-Since&lt;/code&gt; to &lt;code&gt;304&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Over the last four-plus years, I have been almost exclusively working on some sort of *-as-a-Service (*aaS)—for example, Mortgage Origination as a Service, Insurance Claims as a Service. I always see a couple of things when implementing Web (HTTP) services: the &lt;em&gt;reinvention of the wheel&lt;/em&gt; and recognizing the problem ETags solves after publishing a specification (sometimes both).&lt;/p&gt;
&lt;p&gt;With *aaS as a Web API, the intention is to have multiple API clients providing access to representations of shared resources. Early in projects like this involves an initial (single) client, so the chances of a client having a stale resource representation are slim. When another client starts to use the API and an update gets accidentally overwritten, things get needlessly complicated.&lt;/p&gt;
&lt;p&gt;I've seen teams address this problem in a number of ways, often involving a date-time stamp. With multiple clients on an API, scalability is an issue, and a date-time stamp can mean different things to different servers (as we'll see below). You need a single authority for a resource's last modified date-time to avoid exchanging one problem for another. See &lt;a href="https://datatracker.ietf.org/doc/html/rfc7232#section-2.2.2"&gt;Last-Modified/Comparison&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;The creators of HTTP encountered this issue and added features to HTTP to deal with this (I assume that's why they added these features). I don't know when these features were devised, but they proposed them in 1997. So, they've been in the wild for at least 25 years with the entire web as a test bed. So, many brilliant people either created or scrutinized the solution. i.e., it's a wheel.&lt;/p&gt;
&lt;p&gt;The HTTP features are &lt;em&gt;ETags&lt;/em&gt; and &lt;em&gt;conditional requests&lt;/em&gt; and enable &lt;em&gt;optimistic concurrency&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="etags"&gt;ETags&lt;/h2&gt;
&lt;p&gt;An &lt;a href="https://datatracker.ietf.org/doc/html/rfc7232#section-2.3"&gt;ETag&lt;/a&gt; (AKA entity-tag) addresses the &amp;quot;lost update&amp;quot; problem where there are two clients of an API that have received the representation of a version of an entity. Still, another client updates the entity before the other: the second update causes the first the be &amp;quot;lost.&amp;quot; See the following diagram for a visualization:&lt;/p&gt;
&lt;p&gt;&lt;img src="./assets/lost-update-sequence.png" class="img-fluid" alt="Lost-update" /&gt;&lt;/p&gt;
&lt;p&gt;An ETag addresses accidental overwrite by &lt;em&gt;versioning&lt;/em&gt; the resource with an entity-tag (a hash of the representation, a version, etc.). When a client requests a resource, the server may include an ETag validator header field with an entity-tag value in the response. The URI of the resource, along with that entity-tag, constitutes an identifier for a particular version of an entity.&lt;/p&gt;
&lt;p&gt;When a client requests a change to the entity, it includes the entity-tag as a basis version with a conditional header field (like &lt;code&gt;If-Match&lt;/code&gt;.)  The server responds with &lt;code&gt;412 (Precondition Failed)&lt;/code&gt;, and the client can retrieve the latest version, re-apply their change, and re-send. See the following diagram for a visualization:&lt;/p&gt;
&lt;p&gt;&lt;img src="./assets/lost-update-solution-sequence.png" class="img-fluid" alt="Lost-update" /&gt;&lt;/p&gt;
&lt;h2 id="falling-back-to-date-and-time"&gt;Falling back to date and time&lt;/h2&gt;
&lt;p&gt;Even if you use date and time, &lt;strong&gt;HTTP also covers you with other precondition header fields involving modification date&lt;/strong&gt;. The &lt;code&gt;If-Unmodified-Since&lt;/code&gt; and &lt;code&gt;If-Modified-Since&lt;/code&gt; precondition header fields allow you to pass modification date preconditions to make a request conditional. When the precondition isn't met, a &lt;code&gt;412 (Precondition Failed)&lt;/code&gt; status code will be in the response, or for &lt;code&gt;GET&lt;/code&gt; or &lt;code&gt;HEAD&lt;/code&gt;, a &lt;code&gt;304 (Not Modified)&lt;/code&gt; status code will be in the response.&lt;/p&gt;
&lt;p&gt;The initial GET of a resource that supports modification dates in conditional requests will include a &lt;code&gt;Last-Modified&lt;/code&gt; header field validator. The &lt;code&gt;Last-Modified&lt;/code&gt; validator is in the form of an &lt;a href="https://datatracker.ietf.org/doc/html/rfc7231#section-7.1.1.1"&gt;HTTP-date&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="being-successful"&gt;Being Successful&lt;/h2&gt;
&lt;p&gt;RFC 7232, the HTTP 1.1 specification, section 2.3 describes the &lt;em&gt;entity-tag&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;An entity-tag is an opaque validator for differentiating between multiple representations of the same resource, regardless of whether those multiple representations are due to resource state changes over time, content negotiation resulting in multiple representations being valid at the same time, or both.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means that the ETag value depends on the content-type, so two &lt;strong&gt;different representations of the same resource should have different ETag values&lt;/strong&gt; (e.g., one gzip encoded, one not.)&lt;/p&gt;
&lt;p&gt;This also means that the ETag value is opaque to requestors but does point out that one of the intents of ETags to be &lt;strong&gt;an alternative to using a date-time stamp due to lack of accuracy&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="patch"&gt;PATCH&lt;/h3&gt;
&lt;p&gt;Using &lt;code&gt;PATCH&lt;/code&gt; with something like &lt;a href="https://jsonpatch.com/"&gt;JSONPatch&lt;/a&gt; may seem to help alleviate conflicts by providing more granularity in what is changing. Technically true, to implement this would be non-trivial. The ETag specifics a tag of that edition of the entire resource, not any one field. While comparing a change against a delta between two editions of a resource (keeping in mind those editions may not be adjacent) might be one technique for dealing with that, &lt;strong&gt;creating deltas between arbitrary versions of the same resource is non-trivial&lt;/strong&gt;. You could introduce that sort of thing. Something like event-sourcing might enable that. But remember that there may be interdependencies between properties of a resource, and just because the current request changes a property that hasn't changed since the resource was retrieved doesn't mean there isn't still a conflict.&lt;/p&gt;
&lt;h3 id="last-modified"&gt;Last-Modified&lt;/h3&gt;
&lt;p&gt;Remember that &lt;code&gt;Last-Modified&lt;/code&gt; uses &lt;a href="https://datatracker.ietf.org/doc/html/rfc7231#section-7.1.1.1"&gt;HTTP-date&lt;/a&gt; format, so &lt;strong&gt;&lt;code&gt;Last-Modified&lt;/code&gt; only supports second granularity&lt;/strong&gt;. With multiple origin servers, more than second granularity may be needed to be accurate 100% of the time.&lt;/p&gt;
&lt;h4 id="if-unmodified-since"&gt;If-Unmodified-Since&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;If-Unmodified-Since&lt;/code&gt; is used with state-changing methods like PUT, POST, DELETE, and PATCH to avoid accidental overrides (lost updates). &lt;code&gt;If-Unmodified-Since&lt;/code&gt; imposes the precondition &lt;em&gt;update this entity only if it hasn't changed since the provided date-time&lt;/em&gt;. &lt;strong&gt;Use &lt;code&gt;If-Unmodified-Since&lt;/code&gt; to avoid lost update problems when second granularity is not a problem&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id="if-modified-since"&gt;If-Modified-Since&lt;/h4&gt;
&lt;p&gt;When used with &lt;code&gt;GET&lt;/code&gt; or &lt;code&gt;HEAD&lt;/code&gt;, the &lt;code&gt;If-Modified-Since&lt;/code&gt; header field imposes the precondition &lt;em&gt;respond with &lt;code&gt;304 (Not Modified)&lt;/code&gt; and not with an entity representation if the modification date of the identified resource is not more recent than the date provided&lt;/em&gt;. &lt;strong&gt;Use &lt;code&gt;If-Modified-Since&lt;/code&gt; to avoid re-transferring the same data&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="conflict"&gt;&lt;code&gt;409 (Conflict)&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;409 (Conflict)&lt;/code&gt; may sound like an appropriate response to a conditional PUT/POST/PATCH request, except that &lt;code&gt;412 (Precondition Failed)&lt;/code&gt; is expected. &lt;strong&gt;Response status code &lt;a href="https://datatracker.ietf.org/doc/html/rfc7231#section-6.5.8"&gt;&lt;code&gt;409&lt;/code&gt;&lt;/a&gt; should be used when something about the current state of the resource means that the server cannot change it&lt;/strong&gt;. Also, if you have chosen not to use HTTP precondition features and have included something &lt;em&gt;in the representation of the entity&lt;/em&gt; for versioning (like last-modified-date, see above), then &lt;code&gt;409 (Conflict)&lt;/code&gt; is appropriate to signify a potential accidental overwrite or lost update.&lt;/p&gt;
&lt;h3 id="leveraging-existing-implementations"&gt;Leveraging Existing Implementations&lt;/h3&gt;
&lt;p&gt;Azure Cosmos DB implements ETags and &lt;code&gt;Last-Modified&lt;/code&gt; to be leveraged to support the versioning of resources in your Web API. Technically the ETag is a version of the representation that Cosmos DB provides, so consider generating a new ETag based on what Cosmos DB provides, especially if you support more than one content-type (like XML). Suppose you have the concept of a database DTO or database models different from your MVC models. In that case, you should consider custom entity-tag generation based on the Cosmos-supplied entity-tag.&lt;/p&gt;
&lt;p&gt;To leverage the Cosmos-supplied entity-tag, retain it and re-send it in any state-changing requests to Cosmos in the &lt;code&gt;If-Match&lt;/code&gt; header field. If the entity-tags do not match, Cosmos DB will respond with &lt;code&gt;412&lt;/code&gt;, and the Cosmos DB library will throw a &lt;code&gt;CosmosException&lt;/code&gt; with &lt;code&gt;StatusCode&lt;/code&gt; == &lt;code&gt;HttpStatusCode.PreconditionFailed&lt;/code&gt;.&lt;/p&gt;
&lt;!--
title Lost Update Problem

participant "Client 1" as Client1
participant "Client 2" as Client2
participant API

Client1-&gt;API:""GET /resource/123""
activate Client1
Client1&lt;--API:""200 OK""\n//resource v1 representation//

create Client2
Client2-&gt;API:""GET /resource/123""
activate Client2
Client2&lt;--API:""200 OK""\n//resource v1 representation//
Client1-&gt;API:""PUT /resource/123""
Client1&lt;--API:""200 OK""\n//resource v2 representation//
deactivateafter Client1
destroyafter Client1

Client2-#red&gt;API:""PUT /resource/123""
note over Client1,API#pink:Client 2 is updating the resource based from **v1**, not **v2**:\n&lt;align:center&gt;the v2 update is "lost" to //Client 2//&lt;/align&gt;
Client2&lt;--API:""200 OK""\n//resource v3 representation//

--&gt;
&lt;!--
title Lost Update Solution

participant "Client 1" as Client1
participant "Client 2" as Client2
participant API

Client1-&gt;API:""GET /resource/123""
activate Client1
Client1&lt;--API:""200 OK""\n//resource v1 representation//

create Client2
Client2-&gt;API:""GET /resource/123""
activate Client2
Client2&lt;--API:""200 OK""\n//resource v1 representation//
Client1-&gt;API:""PUT /resource/123\nIf-Match: v1""
Client1&lt;--API:""200 OK""\n//resource v2 representation//
deactivateafter Client1
destroyafter Client1

Client2-&gt;API:""PUT /resource/123\nIf-Match: v1""

Client2&lt;--API:&lt;color:#red&gt;""412 Precondition Failed\nBasis version of resource is out of date""&lt;/color&gt;
--&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="../assets/accidental-overwrite.jpg" class="img-fluid" alt="A stuffed tiger corrupted appearance due to accidental overwrite"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/Being-Successful-With-Domain-Driven-Design--Minimal-Complexity-Part-3</id>
		<title>Being Successful with Domain-Driven Design: Minimal Complexity, Part 3</title>
		<link href="http://blog.peterritchie.com/posts/Being-Successful-With-Domain-Driven-Design--Minimal-Complexity-Part-3" />
		<updated>2023-06-12T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="/assets/complex-relationships.jpg" class="img-fluid" alt="complex-relationships" /&gt;&lt;/p&gt;
&lt;p&gt;With a name like &amp;quot;Domain-Driven Design,&amp;quot; it should be no surprise there is a major focus on the domain and has a huge influence on implementation. We've focused mostly on strategic design patterns and practices like Ubiquitous Language, Bounded Context, etc. But I've also covered a bit of tactical design and implementation. I've transitioned from strategical patterns--that deal with being explicit with domain concepts (Ubiquitous Language, Bounded Contexts)--to tactical patterns that have focused on directly translating domain concepts into code structure or coding patterns (like Services and Aggregates.)&lt;/p&gt;
&lt;p&gt;The concepts and their consistency boundaries are only a couple of things that contribute to the complexity of non-trivial domains. For example, the work required to implement a domain is independent of its concepts and consistency boundaries. Additionally, the system's quality attributes and technical constraints are major influencers on the internal structure of that system. The next set of Domain-Driven Design patterns I'll get into (&lt;em&gt;tactical&lt;/em&gt; patterns) aid in this respect. As we get closer to implementation, the focus turns more towards isolating domain complexity from implementation complexities.&lt;/p&gt;
&lt;p&gt;As with many things in Domain-Driven Design, &lt;em&gt;x&lt;/em&gt; for the sake of &lt;em&gt;x&lt;/em&gt; is not the intention. Many things in most methodologies can be regurgitated and used by rote, providing little to no value. The principles and practices in Domain-Driven Design are best utilized with purpose and intent. Architectural layering is a good example. Each layer needs a reason for being (a purpose) with unidirectional  independence of its concepts from another layer's concepts. Just any two groupings won't do; without the purposeful intent of having two layers with a unidirectional dependency, you'll never gain the benefits of layering. You end up with the added burden of managing a structure that does not give you any layered benefits.&lt;/p&gt;
&lt;p&gt;The minimum complexity for layers is that two groups of concepts (contexts) are uni-directionally interdependent. In Domain-Driven Design, those layers focus on isolating the concerns of a User Interface, Application, Domain, and Infrastructure.&lt;/p&gt;
&lt;p&gt;The Application layer may seem unique to Domain-Driven Design. There are few patterns/methodologies that isolates the concern that the application layer deals with. Ports and Adapters (Hexagonal) and, by extension, Clean Architecture recognize and isolate high-level &lt;em&gt;use cases&lt;/em&gt; from both the domain and the implementation details of a UI. This is the role of the Application layer in Domain-Driven Design to further isolate the domain from how any use case uses the domain (a use case &lt;em&gt;applies&lt;/em&gt; or &lt;em&gt;realizes&lt;/em&gt; the domain). In Ports and Adapters, uses-cases (or, as Cockburn describes, uses-cases) are sequences of interactions between the system and users/actors. With the recognition of these interactions, they can now be isolated as collaborations within the Application layer.&lt;/p&gt;
&lt;p&gt;There are other patterns that isolate interaction behavior structurally within collaborations like the Adapter pattern, but I'll save that for another day.&lt;/p&gt;
&lt;p&gt;A UI may have to deal with different form factors, communication protocols, execution contexts, etc. A loosely coupled UI involves designing an interface that takes all of those things into consideration to be successful. A web-based UI requires a backend that supports open protocols and standards. Protocols and standards relating to implementation or delivery are merely constraints on how a system is implemented. At some level, the domain needs to operate correctly regardless of those constraints.&lt;/p&gt;
&lt;p&gt;Layers are like different team roles, all working together simultaneously to accomplish specific types of goals. Bounded contexts are also like multiple teams, sometimes like a night shift and a day shift or an on-shore and an off-shore team. These types of teams work with some level of independence: shifts may never work together simultaneously, and on- and off-shore teams only work together for a brief time with much more structured communication.&lt;/p&gt;
&lt;p&gt;Recognizing and planning for how teams contribute to the same goals is key for these teams to be effective. It's the recognition that different parts of a larger system need different levels of independence. With teams, this is to utilize resources effectively: like how shifts can use limited resources (human skills) across more of the day (e.g., 24 hours instead of 8.)  Conway's law is just an observation (i.e., a reality). A team (or teams) structure imposes a means and cadence to communication. How often and the way inter-team communications occurs has implicit limits on that communication.&lt;/p&gt;
&lt;p&gt;Recognizing and working with that communications structure can make teams much more successful. Domain-Driven Design makes domains and sub-domains first-class citizens within the practices. Many aspects of architectural and social boundaries can affect the release of a product. A Bounded Context is more than a consistency boundary or scope of a domain model. A Bounded Context also involves work products (deployments, deliverables) and team organization.&lt;/p&gt;
&lt;p&gt;For example, the consistency boundary of a mortgage loan application becoming complete and submitted is a fairly obvious boundary and context. Still, the amount of work involved to support that might be fairly large. The number of people implementing and supporting that context might amount to several teams. The complexity of dealing with several teams of people to deliver parts of the same system can be enormous. Domain-Driven Design also gives us some patterns and practices to address those complexities. You may need to split a domain into more bounded contexts because one context is too complex for a single team to manage. When we start to talk about separating work across teams, we're still talking about bounded contexts. For similar reasons, you may need to split a domain into more bounded contexts (and thus &amp;quot;sub-domains&amp;quot;) simply because of an existing team or reporting structure.&lt;/p&gt;
&lt;p&gt;For delivery to be more successful, it's important to recognize the different teams, reporting structures, team motivations, and missions within the strategic design of the Bounded Contexts. How two teams and how the work product of those two teams interact is unique. Fortunately, there are some patterns to address the dependencies between two teams and their work products that help us address their inherent complexities. I'm assuming there's always some degree of interdependency and independence, and I'm ignoring mutually independent (Separate Ways) and Big Ball of Mud relationships/structures.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It's worth noting that as soon as two contexts are recognized the need to translate between the two becomes a reality. As context become complex to the point of being bounded context so too does the need to recognize and isolate translation. Much of what we do in Domain-Driven Design is the isolation of concepts, concerns, responsibilities, etc. The need for a translation layer is no different.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is a spectrum to the degree of independence of two teams and or the independence of their work products. The teams are very dependent on one end, while the other is extremely independent. With Domain-Driven Design, very dependent teams exhibit a lot of domain overlap. With a lot of domain overlap, you can have an interdependence where teams work as equals or partners. This partnership can manifest in an early re-org of people working on existing or legacy systems. That partnership may start with different teams working on separate parts of the codebase. This partnership may only be one step in the evolution of the teams; the next step is often to organize teams toward the Shared Kernel model.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A spectrum of options is a synonym for &lt;em&gt;infinite combinations&lt;/em&gt;. It's nice to have flexibility, but an endless set of possibilities is hard to map to a finite set of patterns, and it's hard to use established practices if every situation is novel. There are some ideas and structures that Domain-Driven Design details to add some granularity to the domain we're modeling so that we can more easily map complexity to the patterns and practices that address them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the Shared Kernel model, the team carves off a separate shared codebase or a shared component to contain all the things that two or more teams will always or almost always mutually require. A Shared Kernel model involves organizational behavior, like specific responsibilities, code areas, accountability, etc. But Shared Kernel is a fairly casual relationship. With more formality between two teams or components, you usually see an Upstream/Downstream relationship form. It is easy to view the users of the Shared Kernel downstream dependents and evolve to a more formal Upstream/Downstream relationship. A Customer/Supplier model may emerge in cases like this.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A shared codebase is more casual than a shared component, but a shared component promotes more independence. At the component level, it's important to ensure that autonomy hasn't allowed the teams to deviate from a shared plot, which Continuous Integration is intended to address. The component should be integrated with client code at every opportunity. The intent of a Published Language is for all contexts to be on the same page in understanding that domain. It's not that all contexts will adopt the published language as their domain, but they know how to translate in and out of their domain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the Customer/Supplier model, one team owns a component the other uses as the consumer of the component's capabilities. The team that owns the component is the Supplier, and the team that uses it is the Customer. With this model comes organizational behavior with more specific responsibilities, more planning, and scheduling. With this increased independence, the supplier team has very specific goals in which the customer team has a stake and influence, represented in a release cadence and a roadmap. The Customer is usually the driver of what capabilities the component provides next. The integration model of Customer/Supplier is usually a web service.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In a Upstream/Downstream model, there will almost always be some form of Published Language--usually more formal than just a description, often a specification. Translation becomes more formal in a Upstream/Downstream model, often resulting in a translation layer. If the Upstream/Downstream relationship is between two Bounded Contexts with a high degree of independence an Anticorruption Layer is used on one side to manage the differences communicating between the two domains.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Shared kernel and two-team customer/supplier relationships often exist due to the reporting structure or that reporting structure was created to split work across two teams. In a more product-focused organization, you may have a Customer/Supplier model with more than one customer. Multi-customer relationships can be witnessed in larger organizations with things like shared libraries. The customers are still driving the capabilities that the component and team provide them, but it can become more formal to manage the unique requirements of different customers. The supplier team is often more organized or formal and may have more of a product strategy with a product vision and mission that helps guide their work.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With more formal relationships come more formal expectations. Those expectations may come in the form of specifications and processes. Continuous Integration is an example of a process that continuously validates integrability. Potentially less formal than a specification may be a Published Language--which in its simplest form is a description of the concepts of a domain. (more complex forms would be varying degrees of specifications.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Communications with a customer/supplier model within the same organization can be informal. What the team is working on and how they interact with customers might be more like partnerships; teams may work closely together to implement and integrate components. The number of customers or distance from customers can impact this informalness. The further away a customer is (different division, different organization, different company) may impose more formality to the relationship. The work product of the supplier team may be viewed more like a product. And while customers may drive that product, it may be much more formal to the point where the component is independent of any single customer. This type of relationship may be structured more like a service with a very specific or well-specified interface. Moving towards a well-specified interface is the intent of an Open Host Service where the component is remotely accessed (a service) with a specified protocol and interface.&lt;/p&gt;
&lt;p&gt;As a customer has less influence on a service, they may become completely dependent on the supplier team to provide the capabilities they require. They accept the risk that the supplier team may not provide the necessary capabilities in the future. This is extreme, and either no organization would accept this risk, or it is a temporary relationship. In reality, the different models aren't mutually exclusive, but there is a tendency towards one of them. e.g., a relationship tends to be less like a customer/supplier relationship and more like one completely conforming to another. The recognition of this relationship is called the Conformist model.&lt;/p&gt;
&lt;p&gt;highlight: Recognize change will happen but don't try to create a design that accommodates all change.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="/assets/complex-relationships.jpg" class="img-fluid" alt="complex-relationships"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/Being-Successful-With-Domain-Driven-Design--Minimal-Complexity-Part-2</id>
		<title>Being Successful with Domain-Driven Design: Minimal Complexity, Part 2</title>
		<link href="http://blog.peterritchie.com/posts/Being-Successful-With-Domain-Driven-Design--Minimal-Complexity-Part-2" />
		<updated>2023-05-29T00:00:00Z</updated>
		<content>&lt;p&gt;In part one, I talked about the complexity in the language used to communicate the domain. Domain-Driven Design (DDD) deals with that complexity by isolating the concepts in a clear language that domain experts understand. Ubiquitous Language helps form the basis of all the other patterns and practices in Domain-Driven Design through the clear isolation of domain &lt;em&gt;concepts&lt;/em&gt;. The DDD pattern language context map provides a good example of isolating concepts (in this case, Domain-Driven Design concepts):&lt;/p&gt;
&lt;p&gt;&lt;img src="/assets/ddd-pattern-language.png" class="img-fluid" alt="ddd-pattern-language" /&gt;&lt;/p&gt;
&lt;p&gt;Isolating individual concepts, naming them, and detailing how they relate allows each to be thought about independently.  We can focus on parts of &amp;quot;Domain-Driven Design&amp;quot; because a Context Map details that isolation.&lt;/p&gt;
&lt;p&gt;I'll dig deeper into the Aggregate and Service patterns in this part two. Aggregate and Service enable and embody major domain concepts.&lt;/p&gt;
&lt;p&gt;An aggregate is the realization of a logical consistency boundary and the operations contributing to that consistency. An aggregate is a composition of several domain objects: At least one entity object and usually several value objects. Each domain object maintains its own consistency (it has invariants.) A date is a composition of a day, month, and year, but February 31, 1981 is not a valid date.  We differentiate an aggregate from any grouping of domain objects because of the invariants and rules beyond that of simply a collection of consistent domain objects. An aggregate models that cross-object consistency requirement.&lt;/p&gt;
&lt;p&gt;Aggregates map to major domain concepts and significant domain behavior associated with a particular domain object (the root). The root is the object of behavior and acts as a gateway to the other objects the aggregate comprises. The root takes on the responsibility of maintaining the consistency of the entire aggregate. The complexity of that composition and the consistency are separated from complexities outside the natural boundary of the aggregate.&lt;/p&gt;
&lt;p&gt;An aggregate is not a design choice; it is a natural role that a major domain entity plays in the domain that requires recognition in a domain model. Some domain entities naturally take on certain behavior that affects other domain objects. All the behavior must happen in certain ways and have expectedly consistent results. Those consistent results (or state) abide by rules and invariants--it's consistent &lt;em&gt;because&lt;/em&gt;... With a loan application, for example, there's no such thing as having negative assets (particularly: it's not a consistent loan application when it contains assets with negative value.) If you &lt;em&gt;owe&lt;/em&gt; money, that's a liability.&lt;/p&gt;
&lt;p&gt;When understanding and modeling a domain, I like to accurately map behavior to domain concepts that logically have that (or any) behavior. Sometimes it's easy to mis-associate behavior with static concepts when those are the major concepts in the domain. A loan, for example, is a major concept in many financial domains, but it does not exhibit behavior; it's static. A loan is the subject of many behaviors in a financial domain but is just a contract (or a specification.) We know we have complexity to deal with. Mis-associating behavior reduces clarity, making things needlessly more complex.&lt;/p&gt;
&lt;p&gt;Starting out understanding a domain, I find focusing primarily on behavior and activities useful. Everything else in a domain is ultimately the subject of a behavior or activity, so I don't model them explicitly. For example, the role of underwriter &lt;em&gt;approves&lt;/em&gt; a loan application. A loan application can be approved when the following rules are satisfied: a) the Debt-To-Income Ratio is below 43%, and b) etc... Debt-To-Income Ratio is modeled as an attribute of a loan application and covered in the activity of Approving a Loan. Information that isn't the object of a behavior also adds needless complexity.&lt;/p&gt;
&lt;p&gt;When certain logic doesn't require a single object or doesn't require a &lt;em&gt;particular state&lt;/em&gt; and requires &lt;em&gt;particular objects&lt;/em&gt;, it might not be accurate to model the logic as part of an &lt;em&gt;aggregate&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A Service is the realization of a collaboration between several objects. The concept of that service exists because it is not the natural behavior of a domain &lt;em&gt;entity&lt;/em&gt;. A Domain Service is the realization of a collaboration between one or more domain entities. It involves business logic and or business rules that may affect the state of those domain entities. An application service is the realization of a collaboration between one or more domain services, domain objects, and an infrastructure service. And an infrastructure service is a collaboration with a framework or the &amp;quot;outside.&amp;quot;  The Service pattern recognizes complexity by isolating logic that is otherwise unrelated.&lt;/p&gt;
&lt;p&gt;Look again at the Domain-Driven Design pattern language. If we always had to deal with all the complexities detailed in that diagram, it would be much harder to get things done promptly. The fact that the concepts are delineated and we can deal with them in isolation simplifies working with them. Separating interaction-only logic into a separate service from the object behavior affecting their state means we can think about and work with those concepts in isolation. Those concepts are now more loosely-coupled, and we obtain the benefits of loose coupling.&lt;/p&gt;
&lt;p&gt;These definitions are easy enough to understand but can be confusing when it comes time to put them into practice. Sometimes the confusion stems from a backward approach to implementing software systems. Teams work backward from patterns when looking for the opportunity to use patterns. It's more successful in understanding the domain and then matching domain concepts to patterns and practices. For example, I've seen people approach a domain with questions like &amp;quot;What are all the entities?&amp;quot; or &amp;quot;What are all the services?&amp;quot;  While we might be able to answer those questions after we've understood the domain and started to design solutions, approaching it from that perspective at that stage of understanding can pervert the interpretation of the domain. Services can be hard to recognize and implement when the domain concepts are not yet clearly isolated.&lt;/p&gt;
&lt;p&gt;Sometimes it can be easy to delineate interaction logic in a collaboration from the business logic; often, it is not. In a financial domain, &lt;em&gt;transferring funds&lt;/em&gt; as a capability can be easily viewed as the behavior of an account, for example. It involves accounts, obviously, so why would it not be an &lt;em&gt;account behavior&lt;/em&gt;? But what happens when transferring funds? The situation's complexity comes from the fact that more than one account is involved, and the consistency of each account needs to be managed independently of the other(s).  A funds transfer succeeds or fails, but has no state of its own--any change in state is encapsulated in the objects participating in the collaboration.&lt;/p&gt;
&lt;p&gt;A clear understanding of the domain concepts is vital to project the isolation of those concepts into design elements more accurately. Modeling domain knowledge requires the delineation and understanding of the concepts as well as correctly associating all the behavior and attributes of those concepts. Maintaining this model is a process of managing complexity, but it only happens in stages. Managing these complexities is an ongoing and iterative process. The more complex a domain is, knowledge fragmentation amongst domain experts is more likely. It's highly unlikely that a single person completely understands the domain. This knowledge fragmentation is one reason we model the domain iteratively, recognizing that understanding evolves over time.&lt;/p&gt;
&lt;p&gt;The Aggregate and Service patterns model parts of the domain similarly but provide a means to recognize separate parts of the domain: independent of the level at which they apply as well as how they affect state.  Service operates at a higher level to model an activity, a collaboration of objects.  An aggregate is the composition of several domain objects that must abide by the same invariants and be consistent in the presence of each other.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;In part one, I talked about the complexity in the language used to communicate the domain. Domain-Driven Design (DDD) deals with that complexity by isolating the concepts in a clear language that domain experts understand. Ubiquitous Language helps form the basis of all the other patterns and practices in Domain-Driven Design through the clear isolation of domain &lt;em&gt;concepts&lt;/em&gt;. The DDD pattern language context map provides a good example of isolating concepts (in this case, Domain-Driven Design concepts):&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/Being-Successful-With-Domain-Driven-Design--Minimal-Complexity-Part-1</id>
		<title>Being Successful With Domain-Driven Design: Minimal Complexity, Part 1</title>
		<link href="http://blog.peterritchie.com/posts/Being-Successful-With-Domain-Driven-Design--Minimal-Complexity-Part-1" />
		<updated>2023-05-19T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="/assets/concepts-contexts-and-boundaries.jpg" class="img-fluid" alt="Concepts, Context, and Boundaries. Abstract Thought" /&gt;
The Domain-Driven Design book (the &amp;quot;Blue Book&amp;quot;) includes &amp;quot;Tackling complexity at the heart of software&amp;quot; in the title. While &amp;quot;complexity&amp;quot; can be subjective, the takeaway is that Domain-Driven Design intends to address complex software systems. The principles and practices in Domain-Driven Design have their complexities, so for Domain-Driven Design to add value, it needs to address existing/expected complexity and attempt to be net-positive for simplicity.&lt;/p&gt;
&lt;p&gt;Interestingly, the title of the Blue Book alludes to questions about &lt;em&gt;what&lt;/em&gt; complexity is at the heart of software. Subjectively subjective, but we can look to the intent of some of the patterns and practices to deduce some types of complexity that Domain-Driven Design adequately addresses in the design of software systems.&lt;/p&gt;
&lt;p&gt;For this series, I'll tranche away some of the patterns as essential complexity (essential complexity of &lt;em&gt;both&lt;/em&gt; Domain-Driven design and the design of almost all software design):  Entities, Value Objects, Modules, Layered Architecture, Factories, and Repositories. Any modular software system must deal with identity, value, creation, and storage. There's nothing new about layered architecture, but Domain-Driven Design does detail the isolation of specific responsibilities (like Domain and Infrastructure) that I'll cover. Practices like Side-Effect Free Functions, Standalone Classes, Intention-Revealing Interfaces, Continuous Integration, Assertions, and Declarative Style are aspects of long-championed techniques like cohesion, loose-coupling, naming standards, or functional programming (in my opinion).&lt;/p&gt;
&lt;p&gt;None of what I've tranched off are unimportant, but they have been tried and true before Domain-Driven Design, and I want to focus on added value in Domain-Driven Design. To that end, I'll focus on Ubiquitous Language, Bounded Context, Context Map, Aggregates, Services, Domain Layer, Generic Subdomains, Segregated Core, Anti-Corruption Layer, and Core Domain; and touch on Evolving Order.&lt;/p&gt;
&lt;h3 id="clean-concepts-contexts-and-boundaries"&gt;Clean Concepts, Contexts, and Boundaries&lt;/h3&gt;
&lt;p&gt;If I had to distill the intent of Domain-Driven Design to a single statement, it might be &amp;quot;be explicit.&amp;quot;  Or, more explicitly: &amp;quot;Be explicit with boundaries.&amp;quot;  Software systems are not the only source (or victim) of complexity. There's a whole science devoted to it: Complex Adaptive Systems. Not to oversimplify Complex Adaptive Systems, but systems with sufficient complexity are inherently unpredictable and exhibit &lt;em&gt;emergent behavior&lt;/em&gt;, among other things. Meaning that complex systems will do what they're going to do, and we can only sometimes predict what they will do. Sometimes that emergent behavior is beneficial; sometimes, it isn't. To make systems more predictable (and get the benefits that provides), we have to reduce complexity. The complexity of complex systems arises from the number of dependencies, relationships, and interactions. Each unbounded interconnection increases complexity exponentially.&lt;/p&gt;
&lt;p&gt;Complex adaptive systems theory is why explicit boundaries are a major aspect of how Domain-Driven Design combats complexity in software to produce more reliable and robust systems. Explicitness is important here; we're not looking for any-old boundaries. We're looking to constrain and isolate areas of the system based on purpose, meaning, and intent. We could chalk this up as simply an exercise in cohesion, but Domain-Driven Design focuses on getting to and clarifying that purpose, meaning, and intent.&lt;/p&gt;
&lt;p&gt;Explicitness starts with unambiguous concepts, descriptions, and terms. If people aren't communicating the same concept things aren't going to get simpler. I speak about &lt;em&gt;Naming Things&lt;/em&gt;, and part of what makes that difficult, I've decided, is language (or English). Stemming from human nature, we try to classify an ever-increasing set of concepts with a finite set of words, syntax, and semantics. The first step to explicit boundaries is the agreement on what they are: agreement on the concepts and to which explicit context they apply—the Ubiquitous Language.&lt;/p&gt;
&lt;p&gt;The value of Ubiquitous Language isn't just that there is an agreed-upon vocabulary. The added value to a Ubiquitous Language is what it accounts for. The Ubiquitous Language recognizes classifications of concepts, classifications common to most software systems. Classifications that the Ubiquitous Language fosters and isolates: individuals, invariants and consistency rules, operations, processes, collaborations, commands, events, views, and values/properties. By &amp;quot;individuals,&amp;quot; I don't just mean people, but anything that exhibits individuality (aka &amp;quot;entity.&amp;quot;)&lt;/p&gt;
&lt;p&gt;Imagine an amorphous &amp;quot;loan&amp;quot; concept in the financial industry. People apply for loans, obtain loans, and pay back loans. Getting a loan involves evaluating personal information (credit rating, assets, liabilities, etc.). Paying back a loan consists of a term, an interest rate, a payment schedule, etc. Credit rating, assets, liabilities, term, interest rate, and payment schedule are six interconnected concepts. With six concepts (with each having five interconnections to the others), there are 30 interconnections. Or 30 complexity points. But, if we think of these six concepts as two different semi-independent contexts: &amp;quot;loan application&amp;quot; and &amp;quot;loan servicing,&amp;quot; we end up with two contexts (and one interconnection) with three concepts (or three interconnections) totaling six interconnections. We've gone from 30 complexity points to 6 simply by defining the actual contexts better. In other words, we're being more explicit.&lt;/p&gt;
&lt;p&gt;Explicitness like this--delineating elements of a set into two groups connected by a specific relationship--is creating a boundary between two contexts. This is a very simple example of Bounded Context. As the name implies, Bounded Context is an explicit contextual boundary: where one context ends and another begins. This is a domain's macro level, recognizing that Loan Servicing can only happen after Loan Application is successful. This particular boundary is based on a temporal or procedural boundary. Phases or steps are a good way of organizing domains into bounded contexts.&lt;/p&gt;
&lt;p&gt;In these two contexts, the word &amp;quot;loan&amp;quot; exists in both. The word &amp;quot;loan&amp;quot; is used in the application context as well as in the servicing context. But, in the application context, the meaning is really &amp;quot;loan application,&amp;quot; and in the servicing context, it really means &amp;quot;serviced loan&amp;quot;. Understanding that servicing depends on an approval event in a loan application phase (or activity) allows us to realize an explicit boundary. Sometimes it's as easy as this, but often it's not. There are other ways of teasing out boundaries (or contexts), almost always involving vocabulary elements.&lt;/p&gt;
&lt;p&gt;Sometimes you've got overloaded terms like &amp;quot;loan&amp;quot;; sometimes, you have different terms like different &lt;em&gt;rules&lt;/em&gt;. Different rules are typically applied in different scenarios or involve different parameters. Different rules offer a window into recognizing different contexts with a boundary in-between. You may recognize concepts like this (events, operations, rules/invariants) from the larger list I mentioned above. A Ubiquitous Language can also account for individuals and entities, commands (often related to an activity), views (reports, screens, results), collaborations, and attributes or properties attributed to individuals and entities. Additionally, attributes or properties can be involved in criteria, and categories or subtypes may group individuals and entities.&lt;/p&gt;
&lt;p&gt;Working towards a Ubiquitous Language is working towards concepts more independent from each other. Independent concepts are themselves individual contexts. Any defined concept has a defined context with understandable boundaries. Keeping the complexity of one context bound from others keeps the essential complexity within that context and reduces the accidental complexity that arises from blended contexts.&lt;/p&gt;
&lt;p&gt;Domain-Driven Design adds value when you have a minimal complexity, when a subject matter has multiple terms per classification. Terms can be classified as entities, processes, phases, events, rules, views, etc. The focus of this post was a level of complexity where boundaries are recognizable in the nuances of the vocabulary. In future posts, I'll dig deeper into different the subject matter (or domain) classifications, how you can isolate the complexities of each, and the parts of Domain-Driven Design that apply.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="/assets/concepts-contexts-and-boundaries.jpg" class="img-fluid" alt="Concepts, Context, and Boundaries. Abstract Thought"&gt;
The Domain-Driven Design book (the "Blue Book") includes "Tackling complexity at the heart of software" in the title. While "complexity" can be subjective, the takeaway is that Domain-Driven Design intends to address complex software systems. The principles and practices in Domain-Driven Design have their complexities, so for Domain-Driven Design to add value, it needs to address existing/expected complexity and attempt to be net-positive for simplicity.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/installing-dotnet-framework-4-5-targeting-pack</id>
		<title>Installing .NET Framework 4.5 Targeting Pack</title>
		<link href="http://blog.peterritchie.com/posts/installing-dotnet-framework-4-5-targeting-pack" />
		<updated>2023-02-05T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="/assets/DALL%C2%B7E-2023-02-05-13.14.52--ludites-frustration-with-errors-in-integrated-development-environments-(IDE)-pencil-and-watercolor.png" class="img-fluid" alt="When working in an IDE seems like working with crayons" /&gt;&lt;/p&gt;
&lt;p&gt;Something came up with a client around Live Dependency Validation in Visual Studio recently.  Digging into it I ran into several issues, one of which was the error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Severity	Code	Description	Project	File	Line	Suppression State
Error		The reference assemblies for .NETFramework,Version=v4.5 were not found. To resolve this, install the Developer Pack (SDK/Targeting Pack) for this framework version or retarget your application. You can download .NET Framework Developer Packs at https://aka.ms/msbuild/developerpacks	DependencyValidation	C:\Program Files\Microsoft Visual Studio\2022\Enterprise\MSBuild\Current\Bin\amd64\Microsoft.Common.CurrentVersion.targets	1229	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;.NET Framework 4.5 has been out of support since 2016, so its targetting pack isn't available for download.  I found a couple blogs posts about editing the modelproj file to add things like &lt;code&gt;ResolveAssemblyReferenceIgnoreTargetFrameworkAttributeVersionMismatch&lt;/code&gt; or a &lt;code&gt;PackageReference&lt;/code&gt; to &lt;code&gt;microsoft.netframework.referenceassemblies.net45&lt;/code&gt; but neither worked.&lt;/p&gt;
&lt;p&gt;One of the features of Visual Studio Installers is that they can be a one-stop-shop for all the things you're going to need to develop software (with or without Visual Studio).  One of these features is to install .NET targeting packs!  Although the latest version of Visual Studio doesn't include out-of-support components, prior versions of Visual Studio are available.  Visual Studio 2019 came out before .NET Framework 4.5 was completely unsupported (i.e. still had the option of paid support) so it offers the ability to install some targetting packs that are currently out of support.&lt;/p&gt;
&lt;p&gt;You can download older versions of Visual Studio via &lt;a href="https://bit.ly/vs-old"&gt;https://visualstudio.microsoft.com/vs/older-downloads/&lt;/a&gt;, which seems to redirect you eventually to Visual Studio Subscriptions downloads.  For our purposes, Visual Studio Community Editions works fine.&lt;/p&gt;
&lt;p&gt;To install, run the Visual Studio installer that you've downloaded (if you already have 2019 installed, run the already installed Visual Studio Installer and click &lt;strong&gt;Modify&lt;/strong&gt;) then click &lt;strong&gt;Continue&lt;/strong&gt; to go past the &lt;em&gt;set up a view things&lt;/em&gt; dialog. (if you have VS 2022 installed, this seems to do nothing.)&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Make sure you don't change any of the &lt;em&gt;workloads&lt;/em&gt; (if you have Visual Studio 2019 install already, some may be checked--don't uncheck them, that will uninstall them).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src="/assets/vs2019-installer-45-targetting-pack.png" class="img-fluid" alt="Visual Studio Installer" /&gt;&lt;/p&gt;
&lt;p&gt;Click on the &lt;strong&gt;Individual Components&lt;/strong&gt; tab at the top (to the right of &lt;em&gt;Workloads&lt;/em&gt; and to the left of &lt;em&gt;Language packs&lt;/em&gt;.)  In the .NET section, find an check &lt;strong&gt;.NET Framework 4.5 targetting pack&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Click &lt;strong&gt;Install&lt;/strong&gt; or &lt;strong&gt;Install while downloading&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Once completed, you now have the .NET Framework 4.5 targetting pack.  If you're doing this in response to a .NET Framework 4.5 targetting pack error message in Visual Studio, exit and re-start Visual Studio--the error should go away (it does with the modelproj error.)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Incidentally, the other issues I encountered are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Full solution analysis for C# is currently disabled. You may not be seeing all possible dependency validation issues in C# projects. Options... Don't show again 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;... with no way to enable full solution analysis in a way that this notice recognizes and goes away.&lt;/p&gt;
&lt;p&gt;I'd appreciate any advice to resolve that that doesn't involve clicking &lt;strong&gt;Don't show again&lt;/strong&gt;.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="/assets/DALL%C2%B7E-2023-02-05-13.14.52--ludites-frustration-with-errors-in-integrated-development-environments-(IDE)-pencil-and-watercolor.png" class="img-fluid" alt="When working in an IDE seems like working with crayons"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/things-i-learned-attempting-azure-administrator-associate-part-2</id>
		<title>Things I Learned Attempting Azure Administrator Associate - Part 2 - Storage</title>
		<link href="http://blog.peterritchie.com/posts/things-i-learned-attempting-azure-administrator-associate-part-2" />
		<updated>2022-12-22T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="/assets/DALL%C2%B7E-2022-12-22-17.03.40--distributed-cloud-data-storage-in-the-style-of-salvator-dali.png" class="img-fluid" alt="distributed cloud data storage" /&gt;&lt;/p&gt;
&lt;p&gt;Azure Administrator Associate certification is about the skills required to be an Azure account, subscription, tenant, etc., administrator. If your end goal is to develop applications on Azure, that involves a lot of &lt;em&gt;administration&lt;/em&gt; of Azure resources. Regardless of your plan, storage administration is nuanced. This post focuses on some of those nuances, nuances that may not be apparent in the documentation.&lt;/p&gt;
&lt;h3 id="overview"&gt;Overview&lt;/h3&gt;
&lt;!--capabilities--&gt;
&lt;p&gt;Azure provides storage services for Files, Blobs, Queues, and Tables. Files are blobs that support access via SMB protocol, AKA File Shares. Blobs are web resources that support access via a URI (HTTP). Blob Storage supports two types of blobs: block blobs and page blobs. Table Storage supports key-based relational and document access. Queues support access to ephemeral messages.&lt;/p&gt;
&lt;p&gt;Azure Files has a File Sync feature that supports file-level replication across Windows Servers. The Azure File endpoint is also called the Cloud Endpoint and is part of a Sync Group that includes one or more Windows Server file shares.&lt;/p&gt;
&lt;p&gt;There are two performance options for Storage Accounts: Standard (general purpose v2) and Premium (for low latency.)&lt;/p&gt;
&lt;!--tiers/skus--&gt;
&lt;p&gt;Storage has a couple of storage tiers: Standard and Premium. Storage tiers provide different functionality at different costs. Blob Storage has several access tiers: Hot, Cool, and Archive. Access tiers offer a way to communicate the frequency and type of data access to reduce storage costs. Access tiers can be used to implement a lifecycle for data, moving to lower-cost tiers over time to reduce cost.&lt;/p&gt;
&lt;!--durability/high-availability--&gt;
&lt;p&gt;Storage supports data redundancy that makes copies of data to avoid loss due to infrastructure failure. There are several options: Locally-Redundant Storage (LRS), Zone-Redundant storage (ZRS), Geo-Redundant Storage (GRS), and Geo-Zone-Redundant Storage (GZRS). LRS stores three copies of the data asynchronously within a single data center. ZRS duplicates those 3 LRS copies across three availability zones (clusters) in a region. GRS duplicates those 3 LRS asynchronously to a single zone in a secondary region. GZRS duplicates the ZRS data across zones within the secondary region.&lt;/p&gt;
&lt;!--data protection--&gt;
&lt;p&gt;Recovery Services is the service responsible for storing backups and recovery points. Recovery Services stores data within Recovery Services Vaults.&lt;/p&gt;
&lt;p&gt;Encryption scopes logically group blobs or containers and assign an encryption key specific to that scope.&lt;/p&gt;
&lt;!--access control--&gt;
&lt;p&gt;There are a couple options for controlling access to data: Azure AD accounts/groups or Shared Access Signatures (SAS). Azure AD Groups provide a more manageable way to control Azure AD account access to data (than simply Azure AD accounts). SAS provides a granular means to provide delegate access to external entities.&lt;/p&gt;
&lt;h3 id="notable-information"&gt;Notable information &lt;!--TIL--&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LRS protects against rack-level hardware failure (so, if you want data-center-wide failure protection, LRS is insufficient).&lt;/li&gt;
&lt;li&gt;LRS is supported for Standard File and Standard Block Blob account types (otherwise, GRS is the default.)&lt;/li&gt;
&lt;li&gt;ZRS protects against data loss due to data center failure (so, if you want region-wide failure protection, ZRS is insufficient)&lt;/li&gt;
&lt;li&gt;GRS protects against region failure.&lt;/li&gt;
&lt;li&gt;GRS and GZRS secondary regions are pre-defined, forcing your data into a specific region.&lt;/li&gt;
&lt;li&gt;GZRS protects against region failure and simultaneous data center failure in the secondary region.&lt;/li&gt;
&lt;li&gt;When defining a data lifecycle, in the case of a tie, the option that results in the least cost will be chosen.&lt;/li&gt;
&lt;li&gt;Migrating from LRS to GRS is supported with a feature called &amp;quot;Live Migration.&amp;quot; Migration from LRS in other scenarios (e.g. to ZRS) must be done manually. Since Premium Storage accounts do not support LRS, Live Migration does not support Premium Storage accounts.&lt;/li&gt;
&lt;li&gt;Live migration supports the use case of a storage account failure of GRS-replicated data. GRS is a second LRS in a secondary region. If a region fails, GRS reduces to LRS, so recovering means using Live Migration.&lt;/li&gt;
&lt;li&gt;Durability is not backup; it provides access to data when infrastructure recoverability isn't an option. Apart from Live Migration, &lt;em&gt;restoration&lt;/em&gt; is limited to manually copying live data when needed.&lt;/li&gt;
&lt;li&gt;Durability does not protect against application-level failure; use backups or custom (application-level) durability in those scenarios.&lt;/li&gt;
&lt;li&gt;Encryption scopes are useful for providing logical data tenancy.&lt;/li&gt;
&lt;li&gt;Files added/modified in a File Share are only detected and replicated to the Windows Server file shares once every 24 hours (i.e., only visible after 24 hours).&lt;/li&gt;
&lt;li&gt;Adding a file share to a Sync Group acts like all the files and folders within the file share were just added, replicating to the cloud endpoint and any other file shares.&lt;/li&gt;
&lt;li&gt;When applying &lt;em&gt;least privilege&lt;/em&gt; to storage accounts, the &lt;strong&gt;Reader&lt;/strong&gt; role is also required on the Azure AD account if the Azure AD account needs to navigate storage resources in the Azure Portal.&lt;/li&gt;
&lt;li&gt;Asynchronous data redundancy options introduce the possibility of data loss. If the asynchronous duplication to the secondary region did not complete, it is out of sync with the last state of the primary region. Application-level logic is required to prevent loss of data in this scenario.&lt;/li&gt;
&lt;li&gt;The Archive tier does not have immediate access; it must be &lt;em&gt;rehydrated&lt;/em&gt; to a cool/hot tier first (usually with a Copy Blob operation of up to 15 hours completion time).&lt;/li&gt;
&lt;li&gt;File Share storage may be backed up to Recover Service vaults, but Blob Storage may not.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This table summarizes the types of storage accounts and the features/redundancy that each support.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Account&lt;/th&gt;
&lt;th&gt;Redundancy&lt;/th&gt;
&lt;th style="text-align: center;"&gt;block blob&lt;/th&gt;
&lt;th style="text-align: center;"&gt;page blob&lt;/th&gt;
&lt;th style="text-align: center;"&gt;append blob&lt;/th&gt;
&lt;th style="text-align: center;"&gt;file share&lt;/th&gt;
&lt;th style="text-align: center;"&gt;queue&lt;/th&gt;
&lt;th style="text-align: center;"&gt;table&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Standard account&lt;/td&gt;
&lt;td&gt;LRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Standard account&lt;/td&gt;
&lt;td&gt;GRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Standard account&lt;/td&gt;
&lt;td&gt;GAZRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Standard account&lt;/td&gt;
&lt;td&gt;RA-GZRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Premium Block blobs account&lt;/td&gt;
&lt;td&gt;LRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Premium Block blobs account&lt;/td&gt;
&lt;td&gt;ZRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Premium File shares account&lt;/td&gt;
&lt;td&gt;LRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Premium File shares account&lt;/td&gt;
&lt;td&gt;ZRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Premium Page blobs account&lt;/td&gt;
&lt;td&gt;LRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Premium Page blobs account&lt;/td&gt;
&lt;td&gt;ZRS&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☑&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;td style="text-align: center;"&gt;☐&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="/assets/DALL%C2%B7E-2022-12-22-17.03.40--distributed-cloud-data-storage-in-the-style-of-salvator-dali.png" class="img-fluid" alt="distributed cloud data storage"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/things-i-learned-attempting-azure-administrator-associate</id>
		<title>Things I Learned Attempting Azure Administrator Associate - Part 1</title>
		<link href="http://blog.peterritchie.com/posts/things-i-learned-attempting-azure-administrator-associate" />
		<updated>2022-12-20T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="/assets/DALL%C2%B7E-2022-12-20-15.03.33---A-woman-going-through-the-process-of-certifying-knowledge.png" class="img-fluid" alt="person going through the process of certifying knowledge" /&gt;&lt;/p&gt;
&lt;p&gt;I recently earned certification for Azure Administrator Associate.  My goal is to make my experience and skills more verifiable in areas like application solution architecture.  Azure Administrator Associate is a prerequisite for Azure Solutions Architect Expert and DevOps Engineer Expert (I imagine it's a prerequisite for all Azure * {Expert|Associate} certs.)&lt;/p&gt;
&lt;p&gt;Certifications aren't perfect, &amp;quot;certification&amp;quot; has different meanings to the observer and the certification itself. Most certifications bring with them an expected minimum understanding of the subject.  Does it mean the earner will do everything perfectly with the subject?  Of course not, but it gives the person a certain vocabulary to communicate more efficiently on the subject.&lt;/p&gt;
&lt;p&gt;The road to Azure Administrator Associate was interesting, and sharing some notable information would be helpful for others.&lt;/p&gt;
&lt;h2 id="making-the-implicit-explicit"&gt;Making The Implicit Explicit&lt;/h2&gt;
&lt;p&gt;The key to good communication is clearly understanding a subject and eliminating assumptions and misunderstandings. While understanding what is expected of a certified Azure Administrator Associate, I noticed some knowledge that I realized is typically implicit. Another way of looking at the following is that each starts with &amp;quot;It may seem obvious, but...&amp;quot;.&lt;/p&gt;
&lt;p&gt;Implicit knowledge is knowledge obtained through incidental activities; knowledge gained without awareness of learning is occurring.&lt;/p&gt;
&lt;h3 id="line-of-business-lob-applications"&gt;Line of Business (LoB) Applications&lt;/h3&gt;
&lt;p&gt;Line of Business (LoB) applications is ubiquitous in the computing industry.  Everyone knows what it &lt;em&gt;means&lt;/em&gt;, but if you ask two people to define it, you'll get more than one answer.  While agreement/standardization on what a LoB application is isn't going to happen any time soon, there are certain truths about LoB applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An in-house, custom web application&lt;/li&gt;
&lt;li&gt;Not accessible via the Internet, either behind a firewall or strict access control (authentication and authorization)&lt;/li&gt;
&lt;li&gt;Access &lt;em&gt;may&lt;/em&gt; occur via an application gateway or load balancer&lt;/li&gt;
&lt;li&gt;Specific to the company, business area, or industry&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="azcopy"&gt;AzCopy&lt;/h3&gt;
&lt;p&gt;AzCopy works with Azure storage but only Azure Blob Storage and Azure Files.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There are many areas of clarification with Azure Administrator Associate.  Future posts on the subject will address clarifications involving important explicit limits, restrictions, constraints, rules, etc.&lt;/p&gt;
&lt;p&gt;Are there other implicit aspects of Azure administration that can be made explicit?&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="/assets/DALL%C2%B7E-2022-12-20-15.03.33---A-woman-going-through-the-process-of-certifying-knowledge.png" class="img-fluid" alt="person going through the process of certifying knowledge"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/fundamental-webapi-integration-tests</id>
		<title>Fundamental ASP.Net Minimal API Integration Tests</title>
		<link href="http://blog.peterritchie.com/posts/fundamental-webapi-integration-tests" />
		<updated>2022-11-03T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="/assets/robotic%20computing%20testing.png" class="img-fluid" alt="Robotic computing testing" /&gt;&lt;/p&gt;
&lt;p&gt;I've been involved with some fairly large projects that involved RESTful APIs. When dealing with multiple team members, multiple teams, and OpenAPI specs, there can be many risks. Even when an OpenAPI specification is generated from source code, what the code does can easily be unaligned with the spec. Luckily the spec is a machine-readable contract of the &lt;em&gt;intent and purpose&lt;/em&gt; of the API.&lt;/p&gt;
&lt;p&gt;Automated testing to the rescue! With ASP.NET, you can inject into and observe the middleware pipeline. ASP.NET integration tests are a common way of verifying the pipeline and how it is used. We can create integration tests that process the OpenAPI spec and verify operations are working as expected in various ways. This article dives into a couple of these ways.&lt;/p&gt;
&lt;h2 id="fundamental-api-integration-tests"&gt;Fundamental API Integration Tests&lt;/h2&gt;
&lt;p&gt;With a functioning Web API and an OpenAPI specification that describes it there are some fundamental things we can verify:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The generated OpenAPI document is valid&lt;/li&gt;
&lt;li&gt;The paths have endpoints implemented&lt;/li&gt;
&lt;li&gt;The operations respond with the correct type of response&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, let's set up our solution, projects, and integration testing scaffolding.&lt;/p&gt;
&lt;h2 id="setting-up-the-solution-and-projects"&gt;Setting Up the Solution and Projects&lt;/h2&gt;
&lt;p&gt;We're dealing with a Web API and integration tests, so let's create a Web API project and make the &lt;code&gt;Program&lt;/code&gt; class &lt;code&gt;public&lt;/code&gt;. You can do that manually in Visual Studio; but for consistency, the CLI is powerful (I'm being intentional with framework versions and some configuration options--appending &lt;code&gt;public partial class Program { }&lt;/code&gt; to Program.cs to make the class public):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;dotnet new solution
dotnet new webapi -o WebApi --use-minimal-apis true --framework net6.0 --use-program-main false
echo public partial class Program { } &amp;gt;&amp;gt; WebApi\Program.cs
dotnet sln add WebApi\WebApi.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we want to add a test project. xUnit is my go-to, so we'll use that and add a reference to the Web API project. Again, in the CLI:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;dotnet new xunit -o IntegrationTests --framework net6.0
del IntegrationTests\UnitTest1.cs
dotnet add IntegrationTests\IntegrationTests.csproj reference WebApi\WebApi.csproj
dotnet sln add IntegrationTests\IntegrationTests.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For ASP.Net integration tests, we will use &lt;code&gt;WebApplicationFactory&amp;lt;T&amp;gt;&lt;/code&gt;, which requires a reference to &lt;code&gt;Microsoft.AspNetCore.Mvc.Testing&lt;/code&gt;. In addition, to process OpenAPI documents, we'll need the &lt;code&gt;Microsoft.OpenApi.Readers&lt;/code&gt; package. Again, via the CLI:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;dotnet add IntegrationTests\IntegrationTests.csproj package Microsoft.OpenApi.Readers
dotnet add IntegrationTests\IntegrationTests.csproj package Microsoft.AspNetCore.Mvc.Testing
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="integration-test-scaffolding"&gt;Integration Test Scaffolding&lt;/h2&gt;
&lt;p&gt;I got into some of the scaffolding of ASP.NET 6 integration tests in &lt;a href="#setting-up-the-solution-and-projects"&gt;Setting Up the Solution and Projects&lt;/a&gt; concerning the required package references. the &lt;code&gt;Microsoft.AspNetCore.Mvc.Testing&lt;/code&gt; package is required so that we may use the &lt;a href="https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.mvc.testing.webapplicationfactory-1?view=aspnetcore-6.0"&gt;&lt;code&gt;WebApplicationFactory&amp;lt;TEntryPoint&amp;gt;&lt;/code&gt;&lt;/a&gt; class--which allows us to bootstrap a web application in memory, specifically for testing.&lt;/p&gt;
&lt;p&gt;We'll use &lt;code&gt;WebApplicationFactory&lt;/code&gt; to create an instance of an &lt;code&gt;HttpClient&lt;/code&gt; test fake that works with our in-memory host. In addition, we'll override &lt;code&gt;WebApplicationFactory&lt;/code&gt; to get at some of the Swashbuckle details from the pipeline. We're interested in the generated OpenAPI document for processing and the name of that document to generate the OpenAPI specification URI for verification. Here's an example of a &lt;code&gt;WebApplicationFactory&lt;/code&gt; implementation that does what we need:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class MyWebApplicationFactory : WebApplicationFactory&amp;lt;Program&amp;gt;
{
	public OpenApiDocument? OpenApiDocument { get; private set; }
	public string OpenApiDocumentName { get; private set; } = string.Empty;

	protected override IHost CreateHost(IHostBuilder builder)
	{
		var host = base.CreateHost(builder);
		using var scope = host.Services.CreateScope();
		var sp = scope.ServiceProvider;
		var swaggerGeneratorOptions = sp.GetRequiredService&amp;lt;IOptions&amp;lt;SwaggerGeneratorOptions&amp;gt;&amp;gt;().Value;
		OpenApiDocumentName = swaggerGeneratorOptions.SwaggerDocs.First().Key ?? string.Empty;
		var swaggerProvider = sp.GetRequiredService&amp;lt;ISwaggerProvider&amp;gt;();
		OpenApiDocument = swaggerProvider.GetSwagger(OpenApiDocumentName);

		return host;
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important parts are the &lt;code&gt;OpenApiDocument&lt;/code&gt; and &lt;code&gt;OpenApiDocumentName&lt;/code&gt; properties.&lt;/p&gt;
&lt;p&gt;Now that we've got integration testing scaffolded let's create a test base class to make creating multiple integration tests clean and tidy.&lt;/p&gt;
&lt;h2 id="some-test-conventions"&gt;Some Test Conventions&lt;/h2&gt;
&lt;p&gt;Automated testing classes and methods offer an opportunity to isolate and categorize tests to reduce work and clarify what is being tested (more importantly, what isn't passing). I tend towards a given/when/then structure when designing tests. The test class encapsulates the given/when (as well as the &lt;em&gt;arrange&lt;/em&gt; from arrange/act/assert) whose name is suffixed with &amp;quot;Should.&amp;quot; Each test method in the class is then given a name that describes the &lt;em&gt;then&lt;/em&gt; condition. I try to ensure that there is one condition and thus one assert per method. YMMV.&lt;/p&gt;
&lt;p&gt;For the tests I want to describe in this article, I've created a base class to encapsulate related given/when scenarios (or &lt;em&gt;shoulds&lt;/em&gt;) that require the details we're accessing with the &lt;code&gt;WebApplicationFactory&amp;lt;Program&amp;gt;&lt;/code&gt; implementation. Naming is hard, so I'm starting simple with a &lt;code&gt;WebApiShouldBase&lt;/code&gt; class that encapsulates the parts we're getting with &lt;code&gt;MyWebApplicationFactory&lt;/code&gt; and an ability to get a stream to the &amp;quot;live&amp;quot; OpenAPI spec document (JSON). It also deals with the responsibility of owning those things (e.g., disposal):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class WebApiShouldBase : IDisposable
{
	private readonly string openApiSpecUriText;

	protected readonly HttpClient WebApiClient;
	protected OpenApiDocument? OpenApiDocument { get; }
	protected Task&amp;lt;Stream&amp;gt; GetOpenApiDocumentStreamAsync() =&amp;gt; WebApiClient.GetStreamAsync(openApiSpecUriText);

	protected WebApiShouldBase()
	{
		var factory = new MyWebApplicationFactory();
		WebApiClient = factory.CreateClient();
		OpenApiDocument = factory.OpenApiDocument;
		this.openApiSpecUriText = $&amp;quot;/swagger/{factory.OpenApiDocumentName}/swagger.json&amp;quot;;
	}

	protected virtual void Dispose(bool isDisposing)
	{
		if (isDisposing)
		{
			Dispose();
		}
	}

	public void Dispose() =&amp;gt; WebApiClient.Dispose();

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important parts are the &lt;code&gt;OpenApiDocument&lt;/code&gt; property which re-surfaces the &lt;code&gt;MyWebApplicationFactory.OpenApiDocument&lt;/code&gt; to implementors, the &lt;code&gt;WebApiClient&lt;/code&gt; property to access the API, and the &lt;code&gt;GetOpenApiDocumentStreamAsync&lt;/code&gt; method that holds the OpenAPI spec document that the API provides. This class hides things like the URI to the swagger.json, the use of &lt;code&gt;MyWebApplicationFactory&lt;/code&gt;, disposal, etc.&lt;/p&gt;
&lt;p&gt;With that, let's start doing some tests!&lt;/p&gt;
&lt;h2 id="verifying-the-generated-openapi-is-valid"&gt;Verifying The Generated OpenAPI Is Valid&lt;/h2&gt;
&lt;p&gt;&amp;quot;Valid&amp;quot; is subjective with OpenAPI. An OpenAPI spec is very &lt;em&gt;forgiving&lt;/em&gt; in allowing for many opinions on what a &lt;em&gt;good&lt;/em&gt; API looks like. I'm not going to go deep on what &lt;em&gt;good&lt;/em&gt; might mean; just dive into facilitating validation of that generated document. The fact that there is an OpenApiDocument instance, and a raw OpenAPI specification, is an implementation detail. We'll use that OpenApiDocument instance shortly, but I want to ensure that the raw document meets some minimum requirements. For this example, the OpenAPI document is processed, not errors we detected, and there &lt;em&gt;are&lt;/em&gt; paths. Very simple:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;	[Fact]
	public async Task ProduceValidOpenApi()
	{
		var readerResult = await new OpenApiStreamReader()
			.ReadAsync(await GetOpenApiDocumentStreamAsync().ConfigureAwait(false)).ConfigureAwait(false);
		Assert.NotNull(OpenApiDocument);
		Assert.NotEmpty(readerResult.OpenApiDocument.Paths);
		Assert.Empty(readerResult.OpenApiDiagnostic.Errors);
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Client requirements can be less strict than development requirements (development objectives), and there may be different subsets of requirements in the case of multiple clients. This example doesn't implement that specifically but does provide the means to do it (by adding distinct test methods.)&lt;/p&gt;
&lt;p&gt;OpenAPI.Net has can do very complex verification and validation, but I expect that sort of testing to be performed at a different level--I want to make sure client-oriented tests are handled here.&lt;/p&gt;
&lt;h2 id="verifying-the-paths-have-endpoints-implemented"&gt;Verifying The Paths Have Endpoints Implemented&lt;/h2&gt;
&lt;p&gt;Publishing an API with paths and operations, and hosting an API that hasn't implemented those operations is silly. So the next test verifies they are implemented (at least the GET operations) as specified:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;	[Fact]
	public async Task EndpointsRespondOkToGet()
	{
		Assert.NotNull(OpenApiDocument);
		var pathsWithGetOperations = OpenApiDocument.Paths.Where(w =&amp;gt; w.Value.Operations.ContainsKey(OperationType.Get));

		foreach (var (requestUriText, _) in pathsWithGetOperations)
		{
			var response = await WebApiClient.GetAsync(requestUriText).ConfigureAwait(false);
			Assert.True(response.IsSuccessStatusCode);
		}
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;GET operations are &lt;em&gt;easy&lt;/em&gt;; they shouldn't have a request body and almost always have a success response specified. In the future, I can dive into other types of operations like POST, how to extract samples from the OpenAPI specification, and how to verify operations with request data and or error responses.&lt;/p&gt;
&lt;h2 id="verifying-the-operations-respond-with-the-correct-type-of-response"&gt;Verifying The Operations Respond With The Correct Type Of Response&lt;/h2&gt;
&lt;p&gt;HTTP, and thus OpenAPI, don't enforce that any operation responds with anything in particular. But, if you're reading &lt;em&gt;this&lt;/em&gt; blog, you are probably of the opinion that given the opportunity to specify behavior, you should be at least as detailed in specifying the type and schema of the responses.  I'll leave out validating response schema in this article, but I will show verifying that each request responds with the correct media type. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;	[Fact]
	public async Task EndpointsRespondWithCorrectMediaTypeToGet()
	{
		Assert.NotNull(OpenApiDocument);
		var pathsWithGetOperations = OpenApiDocument.Paths.Where(w =&amp;gt; w.Value.Operations.ContainsKey(OperationType.Get));

		foreach (var (requestUriText, pathItem) in pathsWithGetOperations)
		{
			var responseContentType = pathItem.Operations[OperationType.Get]
				.Responses[OkResponseCodeText]
				.Content
				.Single().Key;

			var request = new HttpRequestMessage
			{
				Method = HttpMethod.Get,
				RequestUri = new Uri(requestUriText, UriKind.Relative),
				Headers =
				{
					{
						HttpRequestHeader.Accept.ToString(),
						responseContentType
					}
				}
			};
			var response = await WebApiClient.SendAsync(request).ConfigureAwait(false);
			Assert.True(response.Content.Headers.ContentType?.MediaType ==
			            responseContentType);
		}
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="caveats"&gt;Caveats&lt;/h2&gt;
&lt;p&gt;Of course, you can have or create an OpenAPI that does little more than document an endpoint and ignore that there are operations and those operations do specific things.&lt;/p&gt;
&lt;p&gt;This article is an overview. I recognize that Swashbuckle and &lt;del&gt;Swagger&lt;/del&gt;OpenAPI support in ASP.NET is powerful, but this article doesn't take into account many things you can do with it (like multiple OpenAPI documents.)&lt;/p&gt;
&lt;p&gt;I also recognize that operations that take no parameters are rare, but I trust that my readers are good with taking on that as an exercise. Or, at least let me know if that's detail I should post in the future.&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;This article provides a very high-level overview of integration testing ASP.NET minimal APIs. We then got into some details of general Web API integration tests that focus on OpenAPI specification aspects of the Web API middleware.&lt;/p&gt;
&lt;p&gt;What sort of automated testing of an API specification do you see as beneficial to your projects?&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/aspnet/core/test/integration-tests?view=aspnetcore-6.0"&gt;Integration tests in ASP.NET Core&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.mvc.testing.webapplicationfactory-1?view=aspnetcore-6.0"&gt;&lt;code&gt;WebApplicationFactory&amp;lt;TEntryPoint&amp;gt;&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The source for the examples, including the creation scripts can be found at &lt;a href="https://github.com/peteraritchie/fundamental-webapi-integration-testing"&gt;https://github.com/peteraritchie/fundamental-webapi-integration-testing&lt;/a&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="/assets/robotic%20computing%20testing.png" class="img-fluid" alt="Robotic computing testing"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/visual-studio-defender-performance</id>
		<title>Visual Studio Performance with Microsoft Defender</title>
		<link href="http://blog.peterritchie.com/posts/visual-studio-defender-performance" />
		<updated>2022-10-27T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img src="/assets/antivirus%20exclusion%20developer.png" class="img-fluid" alt="Antivirus Exclusion Developer Dall-E image" /&gt;&lt;/p&gt;
&lt;p&gt;Steve Smith posted about &lt;a href="https://ardalis.com/speed-up-visual-studio-build-times/?utm_sq=h3m43zzlkm"&gt;speeding up built times in Visual Studio&lt;/a&gt; by configuring Windows Defender.  That was in 2016 and to say things have changed a bit is probably an understatement.  Configuring a new laptop, I thought I'd revisit this briefly.&lt;/p&gt;
&lt;p&gt;Before changing anything in Windows Virus &amp;amp; Threat Protection, go ahead and run a scan to make sure we're starting with a clean slate.  &lt;del&gt;Go to &lt;a href="ms-settings:windowsdefender"&gt;Windows Security&lt;/a&gt; and click &lt;strong&gt;Virus &amp;amp; threat protection&lt;/strong&gt; then click the &lt;strong&gt;Quick scan&lt;/strong&gt; button.&lt;/del&gt; I've been advocating scripting all-the-things, to run a quick scan in an administrator Powershell terminal run &lt;code&gt;Start-MpScan -ScanType QuickScan&lt;/code&gt;.  You can also run a full-scan, if that makes you more comfortable: &lt;code&gt;Start-MpScan -ScanType FullScan&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once that's complete we can configure Windows Virus and Threat Protection to &amp;quot;trust&amp;quot; (exclude) Visual Studio.  To do that in PowerShell you can use the &lt;a href="https://learn.microsoft.com/en-us/powershell/module/defender/add-mppreference?view=windowsserver2022-ps"&gt;&lt;code&gt;App-MpPreference&lt;/code&gt; cmdlet&lt;/a&gt; (as well as see what's already configured with the &lt;a href="https://learn.microsoft.com/en-us/powershell/module/defender/get-mppreference?view=windowsserver2022-ps"&gt;&lt;code&gt;Get-MpPreference&lt;/code&gt; cmdlet&lt;/a&gt;).  Some examples:&lt;/p&gt;
&lt;p&gt;With Visual Studio 2022 Enterprise:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;Add-MpPreference -ExclusionProcess &amp;quot;$Env:ProgramFiles\Microsoft Visual Studio\2022\Enterprise\Common7\IDE\devenv.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With Visual Studio 2022 Professional:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;Add-MpPreference -ExclusionProcess &amp;quot;$Env:ProgramFiles\Microsoft Visual Studio\2022\Professional\Common7\IDE\devenv.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With Visual Studio 2022 Community:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;Add-MpPreference -ExclusionProcess &amp;quot;$Env:ProgramFiles\Microsoft Visual Studio\2022\Community\Common7\IDE\devenv.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And, with Visual Studio 2022 Preview:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;Add-MpPreference -ExclusionProcess &amp;quot;$Env:ProgramFiles\Microsoft Visual Studio\2022\Preview\Common7\IDE\devenv.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also do that for Visual Studio Code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;Add-MpPreference -ExclusionProcess &amp;quot;$Env:LocalAppData\Programs\Microsoft VS Code\code.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also exclude the location of where you store your source code.  The default location is &lt;code&gt;C:\Users\&amp;lt;user-name&amp;gt;\source\repos&lt;/code&gt; for Visual Studio.  So, in PowerShell, you can add a path exclusion:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;Add-MpPreference -ExclusionPath &amp;quot;$Env:USERPROFILE\source\repos&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Note:&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;If you're working with Git repositories that you're unsure of what they contain, you may want to separate where you clone those repos from where you exclude.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Or, if you want a PowerShell script to just do all thee things, see &lt;a href="https://gist.githubusercontent.com/peteraritchie/d6025591566821b4ae5995eb831b6e8d/raw/912b5b20b749d506562437f40e169e6a3e24d279/optimize-defender.ps1"&gt;optimize-defender.ps1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Other processes to consider:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-PowerShell"&gt;Add-MpPreference -ExclusionProcess &amp;quot;$Env:ProgramFiles\dotnet\dotnet.exe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any other processes or paths that you'd consider for exclusion?&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img src="/assets/antivirus%20exclusion%20developer.png" class="img-fluid" alt="Antivirus Exclusion Developer Dall-E image"&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/By-Reference-in-csharp</id>
		<title>By Reference in C#</title>
		<link href="http://blog.peterritchie.com/posts/By-Reference-in-csharp" />
		<updated>2022-09-28T00:00:00Z</updated>
		<content>&lt;p&gt;I became aware recently that there were many C# compiler errors that do not have a corresponding documentation page.  That documentation is open-source and I chose to spend some time contributing some pages for the community. Looking at a language feature from the perspective of its compile-time errors is rather enlightening, so I'd though I'd write a bit about these features in hopes of offering a better understanding for my readers.&lt;/p&gt;
&lt;p&gt;C# compiler errors can be categorized (arbitrarily) by different areas of C# syntax, and I started to focus on one category at a time.  One of those areas involves referenced variables.  C# has always has &lt;code&gt;ref&lt;/code&gt; arguments, but &lt;code&gt;ref&lt;/code&gt; return, &lt;code&gt;ref&lt;/code&gt; locals, &lt;code&gt;ref&lt;/code&gt; structs, and &lt;code&gt;ref&lt;/code&gt; fields have been additions to the syntax.&lt;/p&gt;
&lt;p&gt;The declaration of a variable in C# influences its syntax in a couple ways: binding and accessibility.  Accessibility is whether an identifier is &lt;em&gt;visible&lt;/em&gt; at compile-time in a given context.  Binding is how an identifier or name is bound at run-time to resources like data and code.  Binding uniquely affect the compile-time correctness of any particular usage of a &lt;code&gt;ref&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;Binding affects the compile-time usage of an identifier because of the run-time lifetime of the resources it is bound to. You're probably familiar with a static method accessing instance data and the errors caused in this context. &lt;code&gt;ref&lt;/code&gt; variables have a similar context when they are stack allocated. Heap-allocated objects (objects bound to the heap) can have their lifetime extended to be long-lived because the heap shares the same lifetime as the application. Variables bound to stack-allocated resources cannot have their lifetime extended beyond a specific scope. The stack is a sequential collection of elements with elements implicitly partitioned by a shared scope. The most recognized scope is probably a method call or method/lambda body. Local variables bound to stack elements do not have a lifetime beyond the method call. A reference to a stack object cannot be assigned to a variable or expression with a broader scope.&lt;/p&gt;
&lt;p&gt;How far the value of an expression can leave the confines of its declaration scope is called &amp;quot;escape scope&amp;quot;.  Sometimes the escape scope is the same as the declaration scope.  The compiler verifies compatible escape scopes during assignment.  For example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;    void M(ref int ra)
    {
        int number = 0;
        ref int rl = ref number;
        if (ra == 0)
        {
            int x = number;
            rl = ref x;
        }
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;x&lt;/code&gt; is local to the &lt;code&gt;if&lt;/code&gt; body, it is bound to the stack, and its escape scope is narrower than &lt;code&gt;ref rl&lt;/code&gt; because &lt;code&gt;ref rl&lt;/code&gt; is declared in the outer scope.  Since &lt;code&gt;ref rl&lt;/code&gt; is an alias to another variable, it cannot reference a variable bound to a resource that will go out of scope before it does.  &lt;code&gt;rl = ref x&lt;/code&gt; results in a compiler error.  If &lt;code&gt;rl&lt;/code&gt; were not a reference to a value type, the assignment would be okay because &lt;code&gt;x&lt;/code&gt; would be bound to heap and have a broader escape scope.&lt;/p&gt;
&lt;p&gt;The compiler also verifies compatible escape scopes when returning values.  For example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;    ref int M(ref int ra)
    {
        int number = 0;
        ref int rl = ref number;
        if (ra == 0)
        {
            ref int x = ref number;
            return ref x;
        }
        return ref ra;
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;return ref x&lt;/code&gt; results in a compiler error because the escape scope of &lt;code&gt;x&lt;/code&gt; is local to the method.  The error message here may not be as clear as the first because it doesn't mention the narrower escape scope.&lt;/p&gt;
&lt;p&gt;There are the basic escape scopes.  A calling method scope, a current method scope, and a return-only scope.&lt;/p&gt;
&lt;p&gt;The calling method scope is a scope outside of the containing method/lambda.  References can reach this scope via either a &lt;code&gt;ref&lt;/code&gt; parameter or a return.&lt;/p&gt;
&lt;p&gt;The current method scope is a scope within a containing method/lambda.&lt;/p&gt;
&lt;p&gt;The return-only scope is a special case for &lt;code&gt;ref struct&lt;/code&gt; types that can only leave the method scope via a return and not through a &lt;code&gt;ref&lt;/code&gt; or &lt;code&gt;out&lt;/code&gt; parameter.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I became aware recently that there were many C# compiler errors that do not have a corresponding documentation page.  That documentation is open-source and I chose to spend some time contributing some pages for the community. Looking at a language feature from the perspective of its compile-time errors is rather enlightening, so I'd though I'd write a bit about these features in hopes of offering a better understanding for my readers.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/the-fundamental-quality-attributes-of-technology-systems</id>
		<title>Fundamental Quality Attributes of Technology Systems</title>
		<link href="http://blog.peterritchie.com/posts/the-fundamental-quality-attributes-of-technology-systems" />
		<updated>2022-09-16T00:00:00Z</updated>
		<content>&lt;p&gt;What are quality attributes? The term &amp;quot;non-functional requirements&amp;quot; has been more prevalent, but that is a technologist's term. The first time you bring up &amp;quot;non-functional requirements&amp;quot; with a customer, there's always confusion, then concern. I've heard more than once, &amp;quot;we want something functional.&amp;quot;&lt;/p&gt;
&lt;p&gt;The most important attribute of a system is its functionality. Functionality is whether a system is &lt;em&gt;fit for purpose&lt;/em&gt;. A system's functionality is what makes it unique, so I'll defer detail on functional attributes for another time. In this post I'll focus this post on cross-cutting quality attributes that permeate all aspects of a solution.&lt;/p&gt;
&lt;p&gt;Quality attributes are the characteristics a system needs to exhibit--qualifications of the system's desired functionality. Quality attributes address customer concerns regarding the degree of success of a system. A customer's concerns of a system are unique and thus precludes having a universal prioritized list of quality attributes.&lt;/p&gt;
&lt;p&gt;It is simple to describe the characteristics a customer expects of a system that provides the features they need:  Customers expect features that operate without fault or error, operate consistently within expectations, operate within resource constraints, and protect from unauthorized access. Translating that into a collection of system-specific measures is an enormous undertaking that cannot be taken lightly.&lt;/p&gt;
&lt;p&gt;Many philosophies about quality attributes (usually termed &amp;quot;quality models&amp;quot;) exist, like FURPS, ISO/IEC 9126/25010, McCall, etc. These models detail several categories that organize the many adjectives that can apply to software systems. Some common categories may include Reliable, Efficient, Maintainable, Secure, etc. I view quality attributes as a palette of possible adjectives; no one list is perfect for every situation. There are top-/high-level categorizations that can apply more broadly. When discussing quality attributes, we use the noun form (Reliability vs. Reliable.) I've landed on the following top-level categories (in no particular order): Performance, Operability, Security, and Dependability.&lt;/p&gt;
&lt;h2 id="dependability-of-features"&gt;Dependability (of features)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;to function, without fault or error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dependability involves concerns such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;availability — readiness for usage&lt;/li&gt;
&lt;li&gt;reliability — continuity of service&lt;/li&gt;
&lt;li&gt;safety — non-occurrence of catastrophic consequences on the environment&lt;/li&gt;
&lt;li&gt;confidentiality — non-occurrence of unauthorized disclosure of information&lt;/li&gt;
&lt;li&gt;integrity — non-occurrence of improper alterations of information&lt;/li&gt;
&lt;li&gt;maintainability — aptitude to undergo repairs and evolution&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="performance-of-execution"&gt;Performance (of execution)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To function within resource constraints (time, compute, storage, memory, network) constraints&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Performance involves concerns such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;latency - the degree of responsiveness.&lt;/li&gt;
&lt;li&gt;throughput - the rate at which work can be performed&lt;/li&gt;
&lt;li&gt;capacity - the amount of work that can be performed&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="operability-of-function"&gt;Operability (of function)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;to become and remain operable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Operability involves concerns such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;deployability - the ability of a system to be put into production&lt;/li&gt;
&lt;li&gt;monitorability - the ability of a system's health and operation to be monitored&lt;/li&gt;
&lt;li&gt;configurability - the ability of a system's behavior to be customized&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="security"&gt;Security&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To ensure authorized usage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Security involves concerns such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Confidentiality - the quality of a system restrict access to information&lt;/li&gt;
&lt;li&gt;Integrity - the quality of a system adhere to accuracy and consistency of information and behavior&lt;/li&gt;
&lt;li&gt;Availability - the quality of a system to provide access to information to those authorized&lt;/li&gt;
&lt;li&gt;accountability - the quality of a system to account for its actions when fulfilling its responsibilities&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because quality attributes address customer concerns, there are overlaps between categories. For example, there's a dependability concern that data integrity does not impact functionality; there's also a security concern that data integrity doesn't result in data loss. Don't let the overlap distract you from what's best for your solution. It will change even if you could pre-define a complex structure of quality attributes most suitable for your solution. Needs change, priorities change, and quality attributes are philosophic influencers to a solution that requires nurturing.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;What are quality attributes? The term "non-functional requirements" has been more prevalent, but that is a technologist's term. The first time you bring up "non-functional requirements" with a customer, there's always confusion, then concern. I've heard more than once, "we want something functional."&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>http://blog.peterritchie.com/posts/naming-things-events-and-action</id>
		<title>Naming Things - Common Actions and Events</title>
		<link href="http://blog.peterritchie.com/posts/naming-things-events-and-action" />
		<updated>2022-09-01T00:00:00Z</updated>
		<content>&lt;p&gt;In this multi-part series on Naming Things, I dig into the benefits of having a clear understanding of common terms and concepts--in this case, common actions and events.&lt;/p&gt;
&lt;p&gt;What does Deleted mean? Is it the same as Removed or Destroyed? What if you want to support soft delete as well as hard delete?&lt;/p&gt;
&lt;p&gt;I want to be clear; these aren't developer decisions. They're developer problems based on a lack of clarity in the customer's domain. A customer likely won't use terms like &amp;quot;soft delete&amp;quot; and &amp;quot;hard delete .&amp;quot;The customer will probably refer to the most common form of delete as &amp;quot;delete.&amp;quot; An architect role is responsible for teasing out the nuances of meaning into terms that the subject matter experts agree upon and getting consensus on usage with the development team.&lt;/p&gt;
&lt;p&gt;Every system involves mutating data and information, yet it can be a common source of confusion regarding naming things. There are multiple types of data changes. Systems can create new data and may add that new data to a collection--physical or logical. Systems may change data, or designers may change the structure of data. Data is often removed--from a particular view or existence.&lt;/p&gt;
&lt;p&gt;English allows us to reuse terms to mean many things. &amp;quot;Delete&amp;quot; and &amp;quot;changed,&amp;quot; for example. There are well-known terms that enable us to communicate intent and consequences easily. But, when we reuse these terms across different contexts with different intents and consequences, we introduce the possibility of confusion, making naming things seem difficult.&lt;/p&gt;
&lt;p&gt;It's important to understand the different intents and resulting consequences to data and attempt to get consensus on names and terms that adequately and uniquely represent these situations.&lt;/p&gt;
&lt;p&gt;&amp;quot;Delete&amp;quot; is a common point of confusion. There may be a need for &lt;em&gt;soft deletes&lt;/em&gt; and &lt;em&gt;hard deletes&lt;/em&gt;; both of WHICH make data inaccessible in a context. But, data may also be moved from one context to another, changing its accessibility but not making it &lt;em&gt;inaccessible&lt;/em&gt;. To use a single term like &amp;quot;delete&amp;quot; for all of these situations leads to confusion and issues in naming things.&lt;/p&gt;
&lt;!--
An important aspect of naming: name the consistency boundary. What is a date? A year, month, and day. If we included time, is it still a date? Typically that would be called date/time.  
--&gt;
&lt;p&gt;Each domain can be different, but the situations I've just described are very common. For those, I start with unique terms for each and work with the subject matter experts to refine them (if needed):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Deleted&lt;/strong&gt; means something is no longer accessible in some context.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Destroyed&lt;/strong&gt; means removed from existence; no possible way to ever get it back.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Removed&lt;/strong&gt; means a thing has been moved out of or removed from a container/collection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inverse terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Created&lt;/strong&gt; signifies something new has come into existence (rather than Added).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Added&lt;/strong&gt;* signifies something has been added to a container or collection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mutating information seems like such a simple concept. But, we often need to know if data changes within the context of other data.  Unique data mutation terms I start out with when working with subject matter experts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Updated&lt;/strong&gt; signifies the value properties or attributes of an existing thing (entity) have been changed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Changed&lt;/strong&gt; signifies an entire thing (entity) has been replaced with another.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="word-form"&gt;Word Form&lt;/h2&gt;
&lt;p&gt;Actions are verbs, and events are past participles constructed from verbs. Most event names are constructed from regular verbs by adding the prefix -ed. Delete + ed: deleted; create + ed: created.  Sometimes events are constructed from irregular verbs and don't end in ed. Bind: bound; drive: driven; sleep: slept.&lt;/p&gt;
&lt;p&gt;Events are not simply verbs in past tense form. An event's context is that it is related to a subject. For example, the event &amp;quot;deleted&amp;quot; involves a subject and is used to describe the current state as a result of a past action. In grammar, this is &lt;em&gt;present perfect tense&lt;/em&gt; and implies an auxiliary verb of &amp;quot;has been .&amp;quot;e.g., &lt;em&gt;The customer record has been deleted&lt;/em&gt; or &lt;em&gt;The customer record is deleted&lt;/em&gt;. Since this details the subject somehow, it is also in a &lt;em&gt;present indicative&lt;/em&gt; form. This detail is important but normally for edge cases. Normal domain narratives should align with this because that's normal language in these scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Match events to actions; don't define events when no action would result in that event.&lt;/li&gt;
&lt;li&gt;Don't assume events always end in &amp;quot;ed.&amp;quot;&lt;/li&gt;
&lt;li&gt;Events are present indicative, past participles, and in the present perfect tense.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do you have other actions and events that you commonly encounter?&lt;/p&gt;
</content>
		<summary>&lt;p&gt;In this multi-part series on Naming Things, I dig into the benefits of having a clear understanding of common terms and concepts--in this case, common actions and events.&lt;/p&gt;</summary>
	</entry>
</feed>